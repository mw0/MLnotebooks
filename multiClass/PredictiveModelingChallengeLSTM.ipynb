{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modeling Challenge LSTM\n",
    "\n",
    "**Mark Wilber**\n",
    "\n",
    "The challenge here is to build a classifier for 56 FDA food safety violation categories, which are very unbalanced (sizes spanning more than 3 orders of magnitude). There are two components/features:\n",
    "\n",
    "* a boolean, `FDAISCRITICAL`, indicating whether the violation is 'critical' or not\n",
    "* a description of the violation, `VIOCOMMENT`, which can range from 0 to 844 'words'\n",
    "  * (It is shown below, that the two instances with no comments can be safely dropped.)\n",
    "\n",
    "This notebook generates TF-IDF features after extracting unigrams and bigrams, and trains models using logistic regression, random forest, linear SVC and complement Naive Bayes to compare f1 scores and training times.\n",
    "\n",
    "<font color='darkgreen'>**As thise notebook is lengthy, readers will find it much easier to navigate with [Jupyter Nbextensions](https://github.com/ipython-contrib/jupyter_contrib_nbextensions) installed, and Table of Contents (2) selected:**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "**Next two lines are useful in the event of external code changes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:00.323989Z",
     "start_time": "2020-04-05T21:22:00.311142Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python imports\n",
    "\n",
    "**Next two lines are for pretty output for all prints in a Pandas cell, not just the last.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:00.364435Z",
     "start_time": "2020-04-05T21:22:00.357798Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`DataSci` contains generally helpful data science stuff, while `plotHelpers` includes plot functions specifically.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:01.746135Z",
     "start_time": "2020-04-05T21:22:00.483616Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('/home/wilber/work/Mlib')\n",
    "sys.path.append('/home/mark/work/Mlib')\n",
    "from utility import DataSci as util\n",
    "from plotHelpers import plotHelpers as ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T22:12:07.934722Z",
     "start_time": "2020-04-06T22:12:07.873656Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import time, asctime, gmtime\n",
    "print(asctime(gmtime()))\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "# from platform import node\n",
    "import os\n",
    "from os.path import exists\n",
    "# import shutil\n",
    "# from glob import glob\n",
    "from random import random\n",
    "from collections import Counter, OrderedDict\n",
    "import gc\t\t# garbage collection module\n",
    "import pathlib\n",
    "import pprint\n",
    "# import pickle\n",
    "import timeit\n",
    "\n",
    "print(\"Python version: \", sys.version_info[:])\n",
    "print(\"Un-versioned imports:\\n\")\n",
    "prefixStr = ''\n",
    "if 'collections' in sys.modules:\n",
    "    print(prefixStr + 'collections', end=\"\")\n",
    "    prefixStr = ', '\n",
    "if 'gc' in sys.modules:\n",
    "    print(prefixStr + 'gc', end=\"\")\n",
    "    prefixStr = ', '\n",
    "if 'glob' in sys.modules:\n",
    "    print(prefixStr + 'glob', end=\"\")\n",
    "    prefixStr = ', '\n",
    "if 'pickle' in sys.modules:\n",
    "    print(prefixStr + 'pickle', end=\"\")\n",
    "    prefixStr = ', '\n",
    "if 'platform' in sys.modules:\n",
    "    print(prefixStr + 'platform', end=\"\")\n",
    "    prefixStr = ', '\n",
    "if 'plotHelpers' in sys.modules:\n",
    "    print(prefixStr + 'plotHelpers', end=\"\")\n",
    "    prefixStr = ', '\n",
    "if 'pprint' in sys.modules:\n",
    "    print(prefixStr + 'pprint', end=\"\")\n",
    "    prefixStr = ', '\n",
    "if 'os' in sys.modules:\n",
    "    print(prefixStr + 'os', end=\"\")\n",
    "    prefixStr = ', '\n",
    "if 'os.path' in sys.modules:\n",
    "    print(prefixStr + 'os.path', end=\"\")\n",
    "    prefixStr = ', '\n",
    "if 'random' in sys.modules:\n",
    "    print(prefixStr + 'random', end=\"\")\n",
    "    prefixStr = ', '\n",
    "if 'shutil' in sys.modules:\n",
    "    print(prefixStr + 'shutil', end=\"\")\n",
    "    prefixStr = ', '\n",
    "if 'sys' in sys.modules:\n",
    "    print(prefixStr + 'sys', end=\"\")\n",
    "    prefixStr = ', '\n",
    "if 'timeit' in sys.modules:\n",
    "    print(prefixStr + 'timeit', end=\"\")\n",
    "    prefixStr = ', '\n",
    "if 'utility' in sys.modules:\n",
    "    print(prefixStr + 'utility', end=\"\")\n",
    "    # prefixStr = ', '\n",
    "\n",
    "duVersion = None\n",
    "from dateutil import __version__ as duVersion\n",
    "from dateutil.parser import parse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "import pydot\n",
    "import graphviz\n",
    "\n",
    "scVersion = None\n",
    "from scipy import __version__ as scVersion\n",
    "import scipy.sparse as sp\n",
    "\n",
    "skVersion = None\n",
    "from sklearn import __version__ as skVersion\n",
    "# from sklearn.feature_extraction import text\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.feature_selection import chi2\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "# from sklearn.svm import LinearSVC, SVC\n",
    "# from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "tfVersion = None\n",
    "from tensorflow import __version__ as tfVersion\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "jlVersion = None\n",
    "from joblib import __version__ as jlVersion\n",
    "from joblib import dump, load\n",
    "\n",
    "import seaborn as sns\n",
    "import colorcet as cc\n",
    "\n",
    "mpVersion = None\n",
    "from matplotlib import __version__ as mpVersion\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n\")\n",
    "if 'colorcet' in sys.modules:\n",
    "    print(f\"colorcet: {cc.__version__}\", end=\"\\t\")\n",
    "if 'dateutil' in sys.modules:\n",
    "    print(f\"dateutil: {duVersion}\", end=\"\\t\")\n",
    "if 'graphviz' in sys.modules:\n",
    "    print(f\"graphviz: {duVersion}\", end=\"\\t\")\n",
    "if 'joblib' in sys.modules:\n",
    "    print(f\"joblib: {jlVersion}\", end=\"\\t\")\n",
    "if 'matplotlib' in sys.modules:\n",
    "    print(f\"matplotlib: {mpVersion}\", end=\"\\t\")\n",
    "if 'numpy' in sys.modules:\n",
    "    print(f\"numpy: {np.__version__}\", end=\"\\t\")\n",
    "if 'pandas' in sys.modules:\n",
    "    print(f\"pandas: {pd.__version__}\", end=\"\\t\")\n",
    "if 'pydot' in sys.modules:\n",
    "    print(f\"pydot: {pd.__version__}\", end=\"\\t\")\n",
    "if 'pyreader' in sys.modules:\n",
    "    print(f\"pyreader: {pyreader.__version__}\", end=\"\\t\")\n",
    "if 'scipy' in sys.modules:\n",
    "    print(f\"scipy: {scVersion}\", end=\"\\t\")\n",
    "if 'seaborn' in sys.modules:\n",
    "    print(f\"seaborn: {sns.__version__}\", end=\"\\t\")\n",
    "if 'sklearn' in sys.modules:\n",
    "    print(f\"sklearn: {skVersion}\", end=\"\\t\")\n",
    "if 'tensorflow' in sys.modules:\n",
    "    print(f\"tensorflow: {tfVersion}\", end=\"\\t\")\n",
    "# if '' in sys.modules:\n",
    "#     print(f\": {.__version__}\", end=\"\\t\")\n",
    "Δt = time() - t0\n",
    "print(f\"\\n\\nΔt: {Δt: 4.1f}s.\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "<a id=\"helper-tokenize\"></a>\n",
    "#### `tokenize()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:03.116905Z",
     "start_time": "2020-04-05T21:22:03.087096Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def tokenize(corpus, vocabSz):\n",
    "    \"\"\"\n",
    "    Generates the vocabulary and the list of list of integers for the input corpus\n",
    "\n",
    "    Help from: https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "\n",
    "    INPUTS:\n",
    "        corpus: list, type(str), containing (short) document strings\n",
    "        vocabSz: (int) Maximum number of words to consider in the vocabulary\n",
    "\n",
    "    RETURNS: List of list of indices for each title in the corpus + Keras sentence tokenizer object\n",
    "\n",
    "    Usage:\n",
    "        listOfListsOfIndices, sentenceTokenizer = tokenize(mySentences, maxVocabCt)\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the sentence tokenizer\n",
    "    sentenceTokenizer = Tokenizer(num_words=vocabSz,\n",
    "                                  filters='!#%()*+,./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                  lower=True,\n",
    "                                  split=' ', char_level=False, oov_token=\"<unkwn>\")\n",
    "\n",
    "    # Keep the double quote, dash, and single quote + & (different from word2vec training: didn't keep `&`)\n",
    "    # oov_token: added to word_index & used to replace out-of-vocab words during text_to_sequence calls\n",
    "    # num_words = maximum number of words to keep, dropping least frequent\n",
    "\n",
    "    # Fit the tokenizer on the input corpus\n",
    "    sentenceTokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    # Transform each text in corpus to a sequence of integers\n",
    "    listOfIndexLists = sentenceTokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    return listOfIndexLists, sentenceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `df2TFdata()`\n",
    "\n",
    "This is modified from [a TensorFlow tutorial](https://www.tensorflow.org/tutorials/structured_data/feature_columns), replacing columnar feature with tokenized text for the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:03.143062Z",
     "start_time": "2020-04-05T21:22:03.118127Z"
    }
   },
   "outputs": [],
   "source": [
    "# def df2TFdata(dataframe, textCol, targetCol, shuffle=True, batchSz=64):\n",
    "#     \"\"\"\n",
    "#     dataframe\t\tpd.DataFrame, containing a column with text, and a target column with labels\n",
    "#     shuffle\t\t\tbool, indicating whether to shuffle the data, default: True\n",
    "#     batchSz\t\t\tint, indicating batch size, default: 64\n",
    "#     \"\"\"\n",
    "\n",
    "#     dataframe = dataframe.copy()\n",
    "#     labels = dataframe.pop(targetCol)\n",
    "#     ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "#     if shuffle:\n",
    "#         ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "#     ds = ds.batch(batch_size)\n",
    "\n",
    "#     return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle the data\n",
    "\n",
    "### Read data into a DataFrame\n",
    "\n",
    "* Have a very quick look at DataFrame characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:15.874248Z",
     "start_time": "2020-04-05T21:22:08.298128Z"
    }
   },
   "outputs": [],
   "source": [
    "fname = \"SelectedInspectionReportData.rds\"\n",
    "\n",
    "t0 = time()\n",
    "result = pyreadr.read_r(fname)\n",
    "df = result[None]\n",
    "df.fda_q_fixed = df.fda_q_fixed.astype('int')\n",
    "df.FDAISCRITICAL = df.FDAISCRITICAL.astype('int')\n",
    "Δt = time() - t0\n",
    "print(f\"\\n\\nΔt: {Δt: 4.1f}s.\")\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:15.921965Z",
     "start_time": "2020-04-05T21:22:15.875449Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head(6).T\n",
    "df.tail(6).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:16.328213Z",
     "start_time": "2020-04-05T21:22:15.923322Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:16.463964Z",
     "start_time": "2020-04-05T21:22:16.329478Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove columns from DataFrame which we won't need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:16.650658Z",
     "start_time": "2020-04-05T21:22:16.465108Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df[['fda_q_fixed', 'VIOCOMMENT', 'FDAISCRITICAL']]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory analysis\n",
    "\n",
    "#### Classes and relative balance\n",
    "\n",
    "* The stuff using patches is for placing counts above each rectangle in the bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:16.821760Z",
     "start_time": "2020-04-05T21:22:16.651809Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FDAcodes = list(set(df['fda_q_fixed'].values))\n",
    "print(FDAcodes)\n",
    "classCts = pd.DataFrame(df['fda_q_fixed'].value_counts())\n",
    "with pd.option_context(\"display.max_columns\", 60):\n",
    "    display(classCts.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:17.726620Z",
     "start_time": "2020-04-05T21:22:16.822869Z"
    }
   },
   "outputs": [],
   "source": [
    "ph.plotValueCounts(df, 'fda_q_fixed', titleText='FDA code frequencies', saveAs='svg', ylim=[0.0, 187500.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The class sizes span nearly 4 orders of magnitude!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:24.995119Z",
     "start_time": "2020-04-05T21:22:17.728095Z"
    }
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "df['commentsWords'] = df['VIOCOMMENT'].apply(lambda s: s.split())\n",
    "t1 = time()\n",
    "Δt = t1 - t0\n",
    "print(f\"Δt: {Δt % 60.0:4.1f}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:25.212264Z",
     "start_time": "2020-04-05T21:22:24.996340Z"
    }
   },
   "outputs": [],
   "source": [
    "comments = list(df['commentsWords'])\n",
    "print(comments[0])\n",
    "print(comments[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distribution of comment lengths\n",
    "\n",
    "* Add length of each comment to DataFrame as `wordFreq` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:25.787297Z",
     "start_time": "2020-04-05T21:22:25.213541Z"
    }
   },
   "outputs": [],
   "source": [
    "wordLens = [len(wordList) for wordList in comments]\n",
    "df['wordFreq'] = wordLens\n",
    "wordFreqMode = df['wordFreq'].mode().values[0]\n",
    "\n",
    "wordCtSorted = sorted(wordLens)\n",
    "print(\"smallest word counts:\\n\", wordCtSorted[:100])\n",
    "print(\"largest word counts:\\n\", wordCtSorted[-101:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detailed histogram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:28.239991Z",
     "start_time": "2020-04-05T21:22:25.788415Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(18, 3.5))\n",
    "\n",
    "ph.detailedHistogram(wordLens, ylabel='frequency', volubility=2,\n",
    "                     titleText=f\"Word counts (max: {wordCtSorted[-1]}, mode: {wordFreqMode})\",\n",
    "                     figName=\"WordCountsHist\", ax=ax, ylim = [0.5, 100000.0], ylog=True, saveAs='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:28.283536Z",
     "start_time": "2020-04-05T21:22:28.241280Z"
    }
   },
   "outputs": [],
   "source": [
    "del wordLens\n",
    "del wordCtSorted\n",
    "del df['commentsWords']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What FDA codes correspond to those comments having `wordFreq== 0`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:28.334954Z",
     "start_time": "2020-04-05T21:22:28.284718Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df['wordFreq']==0]\n",
    "print(\"\\n\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can safely remove a couple of records from the 2nd-most populated category**\n",
    "\n",
    "* Originally there were 1307986 records in `df`, out of which 122314 were in Class 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:28.440016Z",
     "start_time": "2020-04-05T21:22:28.336046Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['wordFreq']!=0]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `wordFreq` percentiles\n",
    "\n",
    "* These show that would get 99% coverage of the comments without truncation if were to use, say, 140-element LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:28.568784Z",
     "start_time": "2020-04-05T21:22:28.441139Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.describe(percentiles=[0.01, 0.05, 0.15, 0.25, 0.5, 0.75, 0.85, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most-common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:29.191280Z",
     "start_time": "2020-04-05T21:22:28.569884Z"
    }
   },
   "outputs": [],
   "source": [
    "allWords = [word for wordList in comments for word in wordList]\t\t# Flatten list of lists of words\n",
    "print(len(comments), len(allWords))\n",
    "\n",
    "print(comments[:5], \"\\n\", allWords[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:33.503183Z",
     "start_time": "2020-04-05T21:22:29.192363Z"
    }
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "wordCtr = Counter(allWords)\n",
    "t1 = time()\n",
    "Δt = t1 - t0\n",
    "print(f\"Δt: {Δt % 60.0:4.1f}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Most common words, after removing stop words\n",
    "\n",
    "*Result looks very plausible*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:33.618141Z",
     "start_time": "2020-04-05T21:22:33.504296Z"
    }
   },
   "outputs": [],
   "source": [
    "stopWords = text.ENGLISH_STOP_WORDS.union(['-'])\n",
    "\n",
    "wcStops = [k for k in wordCtr if k.lower() in stopWords]\n",
    "for k in wcStops:\n",
    "    del wordCtr[k]\n",
    "wordCtr.most_common(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:33.854149Z",
     "start_time": "2020-04-05T21:22:33.619474Z"
    }
   },
   "outputs": [],
   "source": [
    "del allWords\n",
    "del wordCtr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fda_q_fixed` vs. `FDAISCRITICAL`\n",
    "\n",
    "What is the relationship between the critical violation boolean and the FDA code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:39.799021Z",
     "start_time": "2020-04-05T21:22:39.536568Z"
    }
   },
   "outputs": [],
   "source": [
    "dfCrit = df.groupby(['fda_q_fixed', 'FDAISCRITICAL']).count()\n",
    "del dfCrit['VIOCOMMENT']\n",
    "del dfCrit['wordFreq']\n",
    "dfCrit.head(20)\n",
    "\n",
    "dfCrit.reset_index(inplace=True)\n",
    "dfCrit.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:22:40.709592Z",
     "start_time": "2020-04-05T21:22:40.255094Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['xtick.top'] = True\n",
    "plt.rcParams['xtick.labeltop'] = True\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "dfCrit.plot.scatter('fda_q_fixed', 'FDAISCRITICAL', s=4, c='black', ax=ax)\n",
    "for xv in np.linspace(0.5, 56.5, 57):\n",
    "    _ = plt.axvline(x=xv, c=\"#FFB0FF\", linewidth=1)\n",
    "plt.suptitle('Critical violations vs FDA code')\n",
    "ax.set_xlim([0.5, 56.5])\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "plt.savefig('CriticalViolationVsFDAcode.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The critical violations plot shows that `FDAISCRITICAL` should be predictive (and certainly should be included in the model):**\n",
    "\n",
    "* **<font color=\"darkgreen\">classes 30, 32, 34 &amp; 46 *never* have critical violations</font>**\n",
    "* **<font color=\"darkgreen\">classes 7, 26, 27 &amp; 29 *only* have critical violations</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "\n",
    "* testFrac\t\t\t\tfraction of data set withheld\n",
    "* maxVocabCt\t\t\tvocabulary size to be returned by Tokenizer, dropping least frequent\n",
    "* LSTMlayerUnits\t\t# units within each activation unit in LSTMs]\n",
    "* embeddingDim\t\t\tsize of dimension for generated embeddings\n",
    "* auxFeaturesCt\t\t\t# of features in auxiliary data\n",
    "* classCt\t\t\t\t# classes (softmax output dim)\n",
    "* dropoutFrac\t\t\tdropout fraction\n",
    "* LSTMdropoutFrac\t\tdropout fraction within LSTMs\n",
    "* batchSz\t\t\t\tsize of batches\n",
    "* epochCt\t\t\t\tnumber of epochs to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:51:36.780130Z",
     "start_time": "2020-04-05T21:51:36.750523Z"
    }
   },
   "outputs": [],
   "source": [
    "testFrac = 0.4\n",
    "maxVocabCt = 80000\n",
    "maxCommentWords = 140\n",
    "LSTMlayerUnits = 64\n",
    "embeddingDim = 64\n",
    "auxFeaturesCt = 1\n",
    "classCt = 56\n",
    "dropoutFrac = 0.15\n",
    "LSTMdropoutFrac = 0.5\n",
    "batchSz = 64\n",
    "epochCt = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data\n",
    "\n",
    "* Split data into train / test sets\n",
    "* Use [helper function `tokenize()`](#helper-tokenize), which invokes Keras Tokenizer, to tokenize sentences\n",
    "  * This will return a list of lists, with each of the latter containing an index for each word in a comment\n",
    "* Finally the input text, `XcommentsTr`, is created by padding / truncating each comment index list to a length of 140.\n",
    "\n",
    "### Split DataFrame by classes\n",
    "\n",
    "* create a `numpy.random.RandomState` instance to keep track of the random number initializer, in order to ensure consistent results throughout\n",
    "\n",
    "* Splitting is done on a per-class basis, so that random selection will not, by chance, yield huge imbalances in train-test splits of tiny classes\n",
    "\n",
    "`splitDataFrameByClasses()` will create two new DataFrames, dfTr, dfTe, according to the desired splits.\n",
    "\n",
    "<font color='darkgreen'><b>Note that if you just want to do stratified sampling on a numpy array of</b> `X` <b>values,</b> `splitDataFramByClasses()` <b>is not needed.</b> `train_test_split()` <b>accepts the keyword</b> `stratify=myTargetVariable`.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:34:59.595787Z",
     "start_time": "2020-04-05T21:34:59.566655Z"
    }
   },
   "outputs": [],
   "source": [
    "randomState=0\n",
    "myRandomState = np.random.RandomState(randomState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:35:02.292723Z",
     "start_time": "2020-04-05T21:35:00.374520Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classColumn = 'fda_q_fixed'\n",
    "dfTr, dfTe = util.splitDataFrameByClasses(df, classColumn,\n",
    "                                          testFrac=testFrac,\n",
    "                                          myRandomState=myRandomState)\n",
    "dfTr.shape, dfTe.shape\n",
    "dfTr.head()\n",
    "dfTe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As intended, `splitBalancedDataFrameClasses()` created new test and train DataFrames, each with ~ 1307984/2 = 653992 rows.**\n",
    "\n",
    "*The test DataFrame is not an exactly split of the original, since the splitting is done by class and unioned. For a 50% split, sci-kit learn's* `train_test_split()` gives the extra instance in each odd-sized class to the test set.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of lists of word indices, and TensorFlow sentence tokenizer object\n",
    "\n",
    "Use comment strings from `dfTrain` to create vocabulary indices.\n",
    "\n",
    "See [helper function `tokenize()`](#helper-tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:35:53.288345Z",
     "start_time": "2020-04-05T21:35:22.289667Z"
    }
   },
   "outputs": [],
   "source": [
    "ListOfCommentsTr = list(dfTr.VIOCOMMENT)\n",
    "\n",
    "listOfListsOfWordIndicesTr, sentenceTokenizer = tokenize(ListOfCommentsTr, maxVocabCt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-pad short comment lists, truncate the ends of long comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:36:02.733327Z",
     "start_time": "2020-04-05T21:35:59.352571Z"
    }
   },
   "outputs": [],
   "source": [
    "# padValue = max(max(listOfListsOfWordIndices)) + 1\n",
    "padValue = 0\n",
    "XcommentsTr = pad_sequences(listOfListsOfWordIndicesTr,\n",
    "                            maxlen=maxCommentWords,\n",
    "                            dtype='int32', padding='pre',\n",
    "                            truncating='post', value=padValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:36:05.941871Z",
     "start_time": "2020-04-05T21:36:05.910185Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ListOfCommentsTr[0]\n",
    "listOfListsOfWordIndicesTr[0]\n",
    "XcommentsTr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary (side) data need to be shaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:36:31.345366Z",
     "start_time": "2020-04-05T21:36:31.315521Z"
    }
   },
   "outputs": [],
   "source": [
    "XauxTr = dfTr.FDAISCRITICAL.values.reshape(dfTr.shape[0], 1)\n",
    "XauxTr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:36:36.095402Z",
     "start_time": "2020-04-05T21:36:36.018560Z"
    }
   },
   "outputs": [],
   "source": [
    "FDAcodesTr = dfTr.fda_q_fixed - 1\n",
    "print(set(FDAcodesTr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor of word indices for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:36:52.848188Z",
     "start_time": "2020-04-05T21:36:40.517571Z"
    }
   },
   "outputs": [],
   "source": [
    "ListOfCommentsTe = list(dfTe.VIOCOMMENT)\n",
    "listOfListsOfWordIndicesTe = sentenceTokenizer.texts_to_sequences(ListOfCommentsTe)\n",
    "XcommentsTe = pad_sequences(listOfListsOfWordIndicesTe,\n",
    "                            maxlen=maxCommentWords,\n",
    "                            dtype='int32', padding='pre',\n",
    "                            truncating='post', value=padValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:36:52.875994Z",
     "start_time": "2020-04-05T21:36:52.849473Z"
    }
   },
   "outputs": [],
   "source": [
    "XauxTe = dfTe.FDAISCRITICAL.values.reshape(dfTe.shape[0], 1)\n",
    "XauxTe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:36:52.935253Z",
     "start_time": "2020-04-05T21:36:52.877381Z"
    }
   },
   "outputs": [],
   "source": [
    "FDAcodesTe = dfTe.fda_q_fixed - 1\n",
    "print(set(FDAcodesTe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model time\n",
    "\n",
    "### Define the model\n",
    "\n",
    "This follows, to some degree, [Keras Multi-Input and multi-output models](https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models)\n",
    "\n",
    "* In this case, we only have a single output\n",
    "* Here, Bidirectional LSTMs are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:37:14.757991Z",
     "start_time": "2020-04-05T21:37:14.724866Z"
    }
   },
   "outputs": [],
   "source": [
    "def buildModel(sequence_length, vocabSz, auxFeatureCount, LSTMinternalLayerSz,\n",
    "               embedLayerDim, densLayerDim=64, softMaxCt=16, dropoutFrac=0.15,\n",
    "               LSTMdropoutFrac=0.40):\n",
    "\n",
    "    \"\"\"\n",
    "    INPUTS:\n",
    "    sequence_length\t\t\tint, number of LSTM units\n",
    "    vocabSz\t\t\t\t\tint, size of vocabulary\n",
    "    auxFeatureCount\t\t\tint, count of auxiliary (side) features\n",
    "    LSTMinternalLayerSz\t\tint, size of layers within LSTM units\n",
    "    embedLayerDim\t\t\tint, dimension of embedding layer\n",
    "    densLayerDim\t\t\tint, dimension of dense layers, default: 64\n",
    "    softMaxCt\t\t\t\tint, dimension of softmax output, default: 16\n",
    "    dropoutFrac\t\t\t\tint, dropout rate, default: 0.15\n",
    "    LSTMdropoutFrac\t\t\tint, dropout rate for LSTMs, default: 0.40\n",
    "    \"\"\"\n",
    "\n",
    "    # Headline input: meant to receive sequences of *sequence_length* integers, between 1 and *vocabSz*.\n",
    "    # Note that we can name any layer by passing it a \"name\" argument.\n",
    "    main_input = Input(shape=(sequence_length,), dtype='int32', name='main_input')\n",
    "\n",
    "    # This embedding layer will encode the input sequence\n",
    "    # into a sequence of dense 64-dimensional vectors.\n",
    "    x = Embedding(output_dim=embedLayerDim, input_dim=vocabSz,\n",
    "                  input_length=sequence_length, trainable=True, name=\"embed_layer\")(main_input)\n",
    "\n",
    "    # A LSTM will transform the vector sequence into a single vector,\n",
    "    # containing information about the entire sequence\n",
    "    lstm_out_1 = Bidirectional(LSTM(LSTMinternalLayerSz,\n",
    "                                    dropout=dropoutFrac,\n",
    "                                    recurrent_dropout=LSTMdropoutFrac,\n",
    "                                    return_sequences=True))(x)\n",
    "    lstm_out = LSTM(LSTMinternalLayerSz,\n",
    "                    dropout=dropoutFrac,\n",
    "                    recurrent_dropout=LSTMdropoutFrac)(lstm_out_1)\n",
    "\n",
    "    auxiliary_input = Input(shape=(auxFeatureCount,), name='numerical_input')\n",
    "    x = concatenate([lstm_out, auxiliary_input])\n",
    "\n",
    "    # We stack a deep densely-connected network on top\n",
    "    x = Dense(densLayerDim, activation='relu')(x)\n",
    "    x = Dense(densLayerDim, activation='relu')(x)\n",
    "\n",
    "    # And finally we add the main logistic regression layer\n",
    "    main_output = Dense(56, activation='softmax', name='main_output')(x)\n",
    "    model = Model(inputs=[main_input, auxiliary_input], outputs=main_output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:37:20.313177Z",
     "start_time": "2020-04-05T21:37:19.639859Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)  # Set a random seed for reproducibility\n",
    "modelLSTM = buildModel(maxCommentWords, maxVocabCt, auxFeaturesCt,\n",
    "                       LSTMlayerUnits, embeddingDim, softMaxCt=classCt)\n",
    "modelLSTM.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute weights for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T10:59:17.238476Z",
     "start_time": "2020-04-06T10:59:17.201731Z"
    }
   },
   "outputs": [],
   "source": [
    "dfTr.fda_q_fixed.unique() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classWeights = class_weight.compute_class_weight('balanced',\n",
    "                                                 dfTr.fda_q_fixed.unique() - 1,\n",
    "                                                 dfTr.fda_q_fixed - 1)\n",
    "print(\"classWeights:\\n\", classWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define callbacks for model checkpoints and TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T20:12:55.231874Z",
     "start_time": "2020-04-06T20:12:55.194289Z"
    }
   },
   "outputs": [],
   "source": [
    "modelInstanceDir = (f\"vocabCt{maxVocabCt:06d}maxCommentLen{maxCommentWords:03d}\"\n",
    "                    + f\"auxFeaturesCt{auxFeaturesCt:02d}classCt{classCt:02d}\"\n",
    "                    + f\"embedDim{embeddingDim:03d}\"\n",
    "                    + f\"LSTMlayerSz{LSTMlayerUnits:03d}batchSz{batchSz:03d}\"\n",
    "                    + f\"dropoutFrac{dropoutFrac:4.2f}\"\n",
    "                    + f\"LSTMdropoutFrac{dropoutFrac:4.2f}\")\n",
    "\n",
    "checkpointDir = './checkpoints'\n",
    "\n",
    "checkpointPrefix = os.path.join(checkpointDir, modelInstanceDir,\n",
    "                                \"ckpt{epoch:03d}\")\n",
    "checkpointCallback=ModelCheckpoint(filepath=checkpointPrefix,\n",
    "                                   save_weights_only=True)\n",
    "\n",
    "tensorBoardLogDir = './tensorBoardLogs'\n",
    "os.makedirs(tensorBoardLogDir, exist_ok=True)                       \n",
    "\n",
    "logsDir = os.path.join(tensorBoardLogDir, modelInstanceDir)\n",
    "\n",
    "os.makedirs(logsDir, exist_ok=True)\n",
    "tensorboardCallback = TensorBoard(log_dir=logsDir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where to stuff plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T20:13:55.840771Z",
     "start_time": "2020-04-06T20:13:55.809153Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotDir = logsDir\n",
    "print(plotDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model graph plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T22:12:14.882862Z",
     "start_time": "2020-04-06T22:12:14.852284Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(modelLSTM, to_file=os.path.join(plotDir, 'modelGraph.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T21:37:31.105361Z",
     "start_time": "2020-04-05T21:37:31.041449Z"
    }
   },
   "outputs": [],
   "source": [
    "modelLSTM.compile(optimizer='rmsprop',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics = ['accuracy', 'categorical_crossentropy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T10:59:17.199999Z",
     "start_time": "2020-04-05T21:56:45.194556Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "history = modelLSTM.fit(x=[XcommentsTr, XauxTr],\n",
    "                        y= FDAcodesTr,\n",
    "                        epochs=epochCt, batch_size=batchSz,\n",
    "                        shuffle=True,\n",
    "                        class_weight=classWeights,\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=[checkpointCallback, tensorboardCallback], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T03:36:00.890085Z",
     "start_time": "2020-04-05T03:35:23.837981Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# history = modelLSTM.fit({'main_input': paddedSequences, 'numerical_input': bools},\n",
    "#                         {'main_output': FDAcodes}, epochs=5, batch_size=batchSz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T20:18:06.718118Z",
     "start_time": "2020-04-07T20:18:06.678875Z"
    }
   },
   "outputs": [],
   "source": [
    "modelLSTM.save(logsDir + '/save.h5', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do inference on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T19:34:28.581041Z",
     "start_time": "2020-04-06T19:19:39.762803Z"
    }
   },
   "outputs": [],
   "source": [
    "softmaxOut = modelLSTM.predict(x=[XcommentsTe, XauxTe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T19:34:28.656967Z",
     "start_time": "2020-04-06T19:34:28.582444Z"
    }
   },
   "outputs": [],
   "source": [
    "yPred = np.argmax(softmaxOut, axis=1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T19:34:28.687598Z",
     "start_time": "2020-04-06T19:34:28.658283Z"
    }
   },
   "outputs": [],
   "source": [
    "dfTe.head(3)\n",
    "dfTe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T19:51:13.676734Z",
     "start_time": "2020-04-06T19:51:13.331644Z"
    }
   },
   "outputs": [],
   "source": [
    "yTe = dfTe.fda_q_fixed\n",
    "confusionMat = confusion_matrix(yTe, yPred)\n",
    "print(confusionMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T19:51:16.063132Z",
     "start_time": "2020-04-06T19:51:16.036797Z"
    }
   },
   "outputs": [],
   "source": [
    "np.where(np.sum(confusionMat, axis=0) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T19:51:17.253106Z",
     "start_time": "2020-04-06T19:51:17.223115Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = np.trace(confusionMat)/np.sum(confusionMat)\n",
    "recall = np.diag(confusionMat)/np.sum(confusionMat, axis=1)\n",
    "precision = np.diag(confusionMat)/np.sum(confusionMat, axis=0)\n",
    "print(f\"accuracy: {accuracy:0.3f}, \"\n",
    "      f\"<precision>: {np.mean(precision):0.3f}, \"\n",
    "      f\"<recall>: {np.mean(recall):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recall, precision by class\n",
    "\n",
    "Note:\n",
    "\n",
    "* `macro avg`: $\\frac{1}{K}\\sum_k m_k$, where $K$ is count of classes and $m_k$ is a given metric for class $k$\n",
    "* `weighted avg`: $\\frac{1}{N}\\sum_k n_k \\cdot m_k$, where $N$ is count of data instance, $n_k$ is the count of points in class $k$ and $m_k$ is a given metric for class $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T19:52:00.953997Z",
     "start_time": "2020-04-06T19:52:00.020922Z"
    }
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(yTe, yPred, target_names=[str(c)for c in FDAcodes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T19:52:14.236179Z",
     "start_time": "2020-04-06T19:52:14.197111Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classCts = dfTe['fda_q_fixed'].value_counts()\n",
    "\n",
    "recall = np.diag(confusionMat)/np.sum(confusionMat, axis = 1)\n",
    "precision = np.diag(confusionMat)/np.sum(confusionMat, axis = 0)\n",
    "f1 = 2.0*precision*recall/(precision + recall)\n",
    "print(\"class\\tprecision\\trecall\\tf1\\tsize\")\n",
    "\n",
    "for FDAcode, classCt in classCts.iteritems():\n",
    "    print(f\"{FDAcode}\\t{precision[FDAcode - 1]:0.3f}\\t\\t{recall[FDAcode - 1]:0.3f}\\t{f1[FDAcode - 1]:0.3f}\\t\\t{classCt:d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot confusion matrix\n",
    "\n",
    "* As this is a straight confusion matrix, diagonal elements mostly reflect class size in test set\n",
    "* *This is hard to interpret by visual inspection alone*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T20:11:16.095343Z",
     "start_time": "2020-04-06T20:11:16.066334Z"
    }
   },
   "outputs": [],
   "source": [
    "labelFontSz = 16\n",
    "tickFontSz = 13\n",
    "titleFontSz = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T20:11:29.038672Z",
     "start_time": "2020-04-06T20:11:16.892781Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(25, 25))\n",
    "ph.plotConfusionMatrix(confusionMat, saveAs='pdf', xlabels=FDAcodes,\n",
    "                       ylabels=FDAcodes, titleText = 'Logistic Regression',\n",
    "                       ax = ax,  xlabelFontSz=labelFontSz, dir=plotDir,\n",
    "                       ylabelFontSz=labelFontSz, xtickFontSz=tickFontSz,\n",
    "                       ytickFontSz=tickFontSz, titleFontSz=titleFontSz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot recall confusion matrix (normalized by row)\n",
    "\n",
    "* diagonal elements now represent the *recall* for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T20:11:44.663216Z",
     "start_time": "2020-04-06T20:11:29.040136Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(25, 25))\n",
    "ph.plotConfusionMatrix(confusionMat, saveAs='pdf', xlabels=FDAcodes, type='recall',\n",
    "                       ylabels=FDAcodes, titleText = 'Logistic Regression',\n",
    "                       ax = ax,  xlabelFontSz=labelFontSz, dir=plotDir,\n",
    "                       ylabelFontSz=labelFontSz, xtickFontSz=tickFontSz,\n",
    "                       ytickFontSz=tickFontSz, titleFontSz=titleFontSz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot precision confusion matrix (normalized by column)\n",
    "\n",
    "* diagonal elements now represent the *precision* for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T20:11:56.123371Z",
     "start_time": "2020-04-06T20:11:44.664743Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(25, 25))\n",
    "ph.plotConfusionMatrix(confusionMat, saveAs='pdf', xlabels=FDAcodes, type='precision',\n",
    "                       ylabels=FDAcodes, titleText = 'Logistic Regression',\n",
    "                       ax = ax,  xlabelFontSz=labelFontSz, dir=plotDir,\n",
    "                       ylabelFontSz=labelFontSz, xtickFontSz=tickFontSz,\n",
    "                       ytickFontSz=tickFontSz, titleFontSz=titleFontSz)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "253.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
