{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `Implicit` and Bayesian Personalized Ranking (BPR) for Recommendation\n",
    "\n",
    "This follow's Ben Frederickson's [Finding Similar Music using Matrix Factorization](https://www.benfrederickson.com/matrix-factorization/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "**Next two lines are for pretty output for all prints in a Pandas cell, not just the last.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:13:06.999680Z",
     "start_time": "2020-04-14T19:13:06.997197Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`DataSci` contains generally helpful data science stuff, while `plotHelpers` includes plot functions specifically.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:13:08.042848Z",
     "start_time": "2020-04-14T19:13:08.040532Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('../../../work/Mlib')\n",
    "# from utility import DataSci as util\n",
    "# from utility import ModelTrain as mt\n",
    "# import plotHelpers as ph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:13:09.881746Z",
     "start_time": "2020-04-14T19:13:09.251990Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import codecs\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sparse\n",
    "import itertools\n",
    "import copy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.approximate_als import (AnnoyAlternatingLeastSquares,\n",
    "                                      FaissAlternatingLeastSquares,\n",
    "                                      NMSLibAlternatingLeastSquares)\n",
    "from implicit.evaluation import (precision_at_k, mean_average_precision_at_k,\n",
    "                                 ndcg_at_k, AUC_at_k, train_test_split)\n",
    "from implicit.bpr import BayesianPersonalizedRanking\n",
    "from implicit.datasets.lastfm import get_lastfm\n",
    "from implicit.datasets.movielens import get_movielens\n",
    "from implicit.datasets.reddit import get_reddit\n",
    "from implicit.datasets.sketchfab import get_sketchfab\n",
    "from implicit.datasets.million_song_dataset import get_msd_taste_profile\n",
    "from implicit.lmf import LogisticMatrixFactorization\n",
    "from implicit.nearest_neighbours import (BM25Recommender, CosineRecommender,\n",
    "                                         TFIDFRecommender, bm25_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:13:11.608782Z",
     "start_time": "2020-04-14T19:13:11.604908Z"
    }
   },
   "outputs": [],
   "source": [
    "models = {\"als\":  AlternatingLeastSquares,\n",
    "          \"nmslib_als\": NMSLibAlternatingLeastSquares,\n",
    "          \"annoy_als\": AnnoyAlternatingLeastSquares,\n",
    "          \"faiss_als\": FaissAlternatingLeastSquares,\n",
    "          \"tfidf\": TFIDFRecommender,\n",
    "          \"cosine\": CosineRecommender,\n",
    "          \"bpr\": BayesianPersonalizedRanking,\n",
    "          \"lmf\": LogisticMatrixFactorization,\n",
    "          \"bm25\": BM25Recommender}\n",
    "\n",
    "dataSets = {\"lastfm\": get_lastfm,\n",
    "            \"movielens\": get_movielens,\n",
    "            \"reddit\": get_reddit,\n",
    "            \"sketchfab\": get_sketchfab,\n",
    "            \"million_song\": get_msd_taste_profile}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `trainTestSplit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:13:14.693035Z",
     "start_time": "2020-04-14T19:13:14.690033Z"
    }
   },
   "outputs": [],
   "source": [
    "# def trainTestSplit(ratings, splitCount, fraction=None):\n",
    "#     \"\"\"\n",
    "#     Stolen from Ethan Rosenthal's Intro to Implicit Matrix Factorization:\n",
    "#     Classic ALS with Sketchfab Models\n",
    "#     https://www.ethanrosenthal.com/2016/10/19/implicit-mf-part-1/\n",
    "\n",
    "#     Split recommendation data into train and test sets\n",
    "\n",
    "#     In order to track precision@k as an optimization metric, it's necessary to only work with .\n",
    "#     A k of 5 would be nice. However, if I move 5 items from training to test for some of the users, then they may not have any data left in the training set (remember they had a minimum 5 likes). Thus, the train_test_split only looks for people who have at least 2*k (10 in this case) likes before moving some of their data to the test set. This obviously biases our cross-validation towards users with more likes. So it goes.\n",
    "\n",
    "#     Params\n",
    "#     ------\n",
    "#     ratings : scipy.sparse matrix\n",
    "#         Interactions between users and items.\n",
    "#     splitCount : int\n",
    "#         Number of user-item-interactions per user to move\n",
    "#         from training to test set.\n",
    "#     fractions : float\n",
    "#         Fraction of users to split off some of their\n",
    "#         interactions into test set. If None, then all\n",
    "#         users are considered.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Note: likely not the fastest way to do things below.\n",
    "#     train = ratings.copy().tocoo()\n",
    "#     test = sparse.lil_matrix(train.shape)\n",
    "\n",
    "#     if fraction:\n",
    "#         try:\n",
    "#             userIndex = np.random.choice(\n",
    "#                 np.where(np.bincount(train.row) >= splitCount*2)[0],\n",
    "#                 replace=False,\n",
    "#                 size=np.int32(np.floor(fraction*train.shape[0]))\n",
    "#             ).tolist()\n",
    "#         except ValueError:\n",
    "#             print(f\"Not enough users with > {2*splitCount} \"\n",
    "#                   f\"interactions to obtain a fraction of {fraction}.\")\n",
    "#         print('Try succeeded!')\n",
    "#     else:\n",
    "#         userIndex = range(train.shape[0])\n",
    "\n",
    "#     train = train.tolil()\n",
    "\n",
    "#     for user in userIndex:\n",
    "#         testRatings = np.random.choice(ratings.getrow(user).indices,\n",
    "#                                        size=splitCount,\n",
    "#                                        replace=False)\n",
    "#         train[user, testRatings] = 0.0\n",
    "#         # These are just 1.0 right now\n",
    "#         test[user, testRatings] = ratings[user, testRatings]\n",
    "\n",
    "#     # Test and training are truly disjoint\n",
    "#     assert(train.multiply(test).nnz == 0)\n",
    "#     return train.tocsr(), test.tocsr(), userIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `calculateSimilarArtists()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:13:16.436109Z",
     "start_time": "2020-04-14T19:13:16.423201Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculateSimilarArtists(outputFilename, dataset, modelName=\"als\"):\n",
    "    \"\"\"\n",
    "    Generates a list of similar artists in lastfm by utilizing the\n",
    "    'similar_items' api of the models\n",
    "    \"\"\"\n",
    "\n",
    "    artists, users, plays = fetchDataset(dataset, volubility=2)\n",
    "    model = getModel(modelName, volubility=2)\n",
    "\n",
    "    # if we're training an ALS based model, weight input for last.fm\n",
    "    # by bm25\n",
    "    if issubclass(model.__class__, AlternatingLeastSquares):\n",
    "        # lets weight these models by bm25weight.\n",
    "        logging.debug(\"weighting matrix by bm25_weight\")\n",
    "        plays = bm25_weight(plays, K1=100, B=0.8)\n",
    "\n",
    "        # also disable building approximate recommend index\n",
    "        model.approximate_recommend = False\n",
    "\n",
    "    # this is actually disturbingly expensive:\n",
    "    plays = plays.tocsr()\n",
    "\n",
    "    logging.debug(\"training model %s\", modelName)\n",
    "    start = time.time()\n",
    "    model.fit(plays)\n",
    "    logging.debug(\"trained model '%s' in %0.2fs\", modelName,\n",
    "                  time.time() - start)\n",
    "\n",
    "    # write out similar artists by popularity\n",
    "    start = time.time()\n",
    "    logging.debug(\"calculating top artists\")\n",
    "\n",
    "    user_count = np.ediff1d(plays.indptr)\n",
    "    to_generate = sorted(np.arange(len(artists)), key=lambda x: -user_count[x])\n",
    "\n",
    "    # write out as a TSV of artistid, otherartistid, score\n",
    "    logging.debug(\"writing similar items\")\n",
    "    with tqdm.tqdm(total=len(to_generate)) as progress:\n",
    "        with codecs.open(outputFilename, \"w\", \"utf8\") as o:\n",
    "            for artistid in to_generate:\n",
    "                artist = artists[artistid]\n",
    "                for other, score in model.similar_items(artistid, 11):\n",
    "                    o.write(\"%s\\t%s\\t%s\\n\" % (artist, artists[other], score))\n",
    "                progress.update(1)\n",
    "\n",
    "    logging.debug(\"generated similar artists in %0.2fs\",  time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `calculateRecommendations()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:13:17.867628Z",
     "start_time": "2020-04-14T19:13:17.858527Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculateRecommendations(outputFilename, modelName=\"als\"):\n",
    "    \"\"\"\n",
    "    Generates artist recommendations for each user in the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    artists, users, plays = fetchDataset(dataset, volubility=2)\n",
    "    model = getModel(modelName, volubility=2)\n",
    "\n",
    "    # if we're training an ALS based model, weight input for last.fm\n",
    "    # by bm25\n",
    "    if issubclass(model.__class__, AlternatingLeastSquares):\n",
    "        # lets weight these models by bm25weight.\n",
    "        logging.debug(\"weighting matrix by bm25_weight\")\n",
    "        plays = bm25_weight(plays, K1=100, B=0.8)\n",
    "\n",
    "        # also disable building approximate recommend index\n",
    "        model.approximate_similar_items = False\n",
    "\n",
    "    # this is actually disturbingly expensive:\n",
    "    plays = plays.tocsr()\n",
    "\n",
    "    logging.debug(\"training model %s\", modelName)\n",
    "    start = time.time()\n",
    "    model.fit(plays)\n",
    "    logging.debug(f\"trained model '{modelName}' in \"\n",
    "                  f\"{time.time() - start:0.2fs}\")\n",
    "\n",
    "    # generate recommendations for each user and write out to a file\n",
    "    start = time.time()\n",
    "    user_plays = plays.T.tocsr()\n",
    "    with tqdm.tqdm(total=len(users)) as progress:\n",
    "        with codecs.open(outputFilename, \"w\", \"utf8\") as o:\n",
    "            for userid, username in enumerate(users):\n",
    "                for artistid, score in model.recommend(userid, user_plays):\n",
    "                    o.write(\"%s\\t%s\\t%s\\n\" % (username, artists[artistid],\n",
    "                                              score))\n",
    "                progress.update(1)\n",
    "    logging.debug(\"generated recommendations in %0.2fs\",  time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser stuff, for command line form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:13:20.373332Z",
     "start_time": "2020-04-14T19:13:20.351830Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "    myDescription = (\"Generates similar artists on the last.fm dataset or \"\n",
    "                     \"generates personalized recommendations for each user.\")\n",
    "    parser = \\\n",
    "        argparse.ArgumentParser(description=myDescription,\n",
    "                                formatter_class=argparse\n",
    "                                .ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "    helpStr = 'Output file name. (Omit to go with parameter-based naming)'\n",
    "    parser.add_argument('--output-base', type=str,  # default='similar-artists'\n",
    "                        dest='outputfile', help=helpStr)\n",
    "    helpStr = f\"model to calculate ({', '.join(models.keys())})\"\n",
    "    parser.add_argument('--model', type=str, default='als',\n",
    "                        dest='model', help=helpStr)\n",
    "    helpStr = f\"dataset ({', '.join(dataSets.keys())})\"\n",
    "    parser.add_argument('--dataset', type=str, default='lastfm',\n",
    "                        dest='dataset', help=helpStr)\n",
    "    helpStr = (\"Recommend items for each user rather than calculate \"\n",
    "               \"similar_items\")\n",
    "    parser.add_argument('--recommend',\n",
    "                        help=helpStr,\n",
    "                        action=\"store_true\")\n",
    "    helpStr = \"Parameters to pass to the model, formatted as 'KEY=VALUE\"\n",
    "    parser.add_argument('--param', action='append',\n",
    "                        help=helpStr)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(\"args:\\n\", args)\n",
    "\n",
    "    if args.outputfile:\n",
    "        outFile = args.outputfile\n",
    "    elif args.recommend:\n",
    "        outFile = f\"recommend-{args.model}-{args.dataset}.tsv\"\n",
    "    else:\n",
    "        outFile = f\"similarItems-{args.model}-{args.dataset}.tsv\"\n",
    "    print(f\"Writing output to {outFile}\")\n",
    "\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "    if args.recommend:\n",
    "        calculateRecommendations(outFile, args.dataset,\n",
    "                                 modelName=args.model)\n",
    "    else:\n",
    "        calculateSimilarArtists(outFile, args.dataset,\n",
    "                                modelName=args.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `getModel()`\n",
    "\n",
    "Creates an instance of one of:\n",
    "\n",
    "|key|model name|\n",
    "|:--|:--|\n",
    "|als|Alternating LeastSquares|\n",
    "|nmslib_als|NMS lib Alternating LeastSquares|\n",
    "|annoy_als|Annoy Alternating LeastSquares|\n",
    "|faiss_als|Faiss Alternating LeastSquares|\n",
    "|tfidf|TF-IDF recommender|\n",
    "|cosine|Cosine recommender|\n",
    "|bpr|Bayesian personalized ranking|\n",
    "|lmf|Logistic matrix factorization|\n",
    "|bm25|BM-25 based recommender|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:13:52.228981Z",
     "start_time": "2020-04-14T19:13:52.222385Z"
    }
   },
   "outputs": [],
   "source": [
    "def getModel(modelName, volubility=1, params=None):\n",
    "    \"\"\"\n",
    "    Instantiates a model class, using provided params, or defaults\n",
    "\n",
    "    INPUTS:\n",
    "        modelName\t\tstr, one of ['als', 'nmslib_als', 'annoy_als',\n",
    "                        'faiss_als', 'tfidf', 'cosine', 'bpr', 'lmf', 'bm25']\n",
    "        params\t\t\tdict, \"suitable\" key-value pairs for model\n",
    "                        description, training conditions, etc.\n",
    "    \"\"\"\n",
    "    if volubility > 0:\n",
    "        print(\"getting model %s\" % modelName)\n",
    "\n",
    "    modelClass = models.get(modelName)\n",
    "    if not modelClass:\n",
    "        raise ValueError(\"Unknown Model '%s'\" % modelName)\n",
    "\n",
    "    # some default params\n",
    "    if not params:\n",
    "        if issubclass(modelClass, AlternatingLeastSquares):\n",
    "            params = {'factors': 32, 'dtype': np.float32}\n",
    "        elif modelName == \"bm25\":\n",
    "            params = {'K1': 100, 'B': 0.5}\n",
    "        elif modelName == \"bpr\":\n",
    "            params = {'factors': 63}\n",
    "        elif modelName == \"lmf\":\n",
    "            params = {'factors': 30, \"iterations\": 40, \"regularization\": 1.5}\n",
    "        else:\n",
    "            params = {}\n",
    "\n",
    "    if volubility > 1:\n",
    "        print(modelName.title)\n",
    "\n",
    "    return modelClass(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fetchDataset()`\n",
    "\n",
    "Get data in convenient sparse matrix format:\n",
    "\n",
    "|key|dataset name|\n",
    "|:--|:--|\n",
    "|lastfm||\n",
    "|movielens||\n",
    "|reddit||\n",
    "|sketchfab||\n",
    "|million_song||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:13:58.611335Z",
     "start_time": "2020-04-14T19:13:58.606369Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetchDataset(dataset, volubility=1):\n",
    "    \"\"\"\n",
    "    If not already in cache directory, /data1/mark/implicit_datasets,\n",
    "    fetches a data set, storing copy in cache.\n",
    "    \n",
    "    INPUT:\n",
    "        dataset\t\tstr, one of ['lastfm', 'movielens', 'reddit',\n",
    "                    'sketchfab', 'million_song']\n",
    "    \"\"\"\n",
    "\n",
    "    if volubility > 0:\n",
    "        print(f\"getting dataset {dataset}\")\n",
    "    getdata = dataSets.get(dataset)\n",
    "\n",
    "    if not getdata:\n",
    "        raise ValueError(f\"Unknown Model {dataset}\")\n",
    "    artists, users, plays = getdata()\n",
    "\n",
    "    if volubility > 1:\n",
    "        print(f\"type(artists): {type(artists)}\")\n",
    "        print(f\"type(users): {type(users)}\")\n",
    "        print(f\"type(plays): {type(plays)}\")\n",
    "\n",
    "    return artists, users, plays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `printLog()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:14:01.689684Z",
     "start_time": "2020-04-14T19:14:01.682037Z"
    }
   },
   "outputs": [],
   "source": [
    "def printLog(row, header=False, spacing=12, outFile=None):\n",
    "    if outFile is None:\n",
    "        outFile = sys.stdout\n",
    "    top = ''\n",
    "    middle = ''\n",
    "    bottom = ''\n",
    "    for r in row:\n",
    "        top += '+{}'.format('-'*spacing)\n",
    "        if isinstance(r, str):\n",
    "            middle += '| {0:^{1}} '.format(r, spacing-2)\n",
    "        elif isinstance(r, int):\n",
    "            middle += '| {0:^{1}} '.format(r, spacing-2)\n",
    "        elif isinstance(r, float):\n",
    "            middle += '| {0:^{1}.5f} '.format(r, spacing-2)\n",
    "        bottom += '+{}'.format('='*spacing)\n",
    "    top += '+'\n",
    "    middle += '|'\n",
    "    bottom += '+'\n",
    "    if header:\n",
    "        outFile.write(top + \"\\n\")\n",
    "        outFile.write(middle + \"\\n\")\n",
    "        outFile.write(bottom + \"\\n\")\n",
    "        outFile.flush()\n",
    "    else:\n",
    "        outFile.write(middle + \"\\n\")\n",
    "        outFile.write(top + \"\\n\")\n",
    "        outFile.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `learningCurve()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:14:26.878332Z",
     "start_time": "2020-04-14T19:14:26.867946Z"
    }
   },
   "outputs": [],
   "source": [
    "def learningCurve(model, train, test, epochs, outFile=None,\n",
    "                  k=5, showProgress=True, numThreads=12):\n",
    "#     if not userIndex:\n",
    "#         userIndex = range(train.shape[0])\n",
    "    prevEpoch = 0\n",
    "\n",
    "    pAtK = []\n",
    "    MAPatK = []\n",
    "    NDCGatK = []\n",
    "    AUCatK = []\n",
    "\n",
    "    headers = ['epochs', f'p@{k}', f'MAP@{k}', f'NDCG@{k}', f'AUC@{k}']\n",
    "    printLog(headers, header=True, outFile=outFile)\n",
    "\n",
    "    for epoch in epochs:\n",
    "        model.iterations = epoch - prevEpoch\n",
    "        if not hasattr(model, 'user_vectors'):\n",
    "            model.fit(train, show_progress=showProgress)\n",
    "        else:\n",
    "            model.fit_partial(train, show_progress=showProgress)\n",
    "        pAtK.append(precision_at_k(model, train.T.tocsr(), test.T.tocsr(),\n",
    "                                   K=k, show_progress=showProgress, num_threads=numThreads))\n",
    "        MAPatK.append(mean_average_precision_at_k(model, train.T.tocsr(), test.T.tocsr(),\n",
    "                                                  K=k, show_progress=showProgress,\n",
    "                                                  num_threads=numThreads))\n",
    "        NDCGatK.append(ndcg_at_k(model, train.T.tocsr(), test.T.tocsr(),\n",
    "                                 K=k, show_progress=showProgress, num_threads=numThreads))\n",
    "        AUCatK.append(AUC_at_k(model, train.T.tocsr(), test.T.tocsr(),\n",
    "                               K=k, show_progress=showProgress, num_threads=numThreads))\n",
    "        row = [epoch, pAtK[-1], MAPatK[-1], NDCGatK[-1], AUCatK[-1]]\n",
    "        printLog(row, outFile=outFile)\n",
    "        prevEpoch = epoch\n",
    "\n",
    "    return model, pAtK, MAPatK, NDCGatK, AUCatK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `gridSearchLearningCurve()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:15:05.994905Z",
     "start_time": "2020-04-14T19:15:05.985700Z"
    }
   },
   "outputs": [],
   "source": [
    "def gridSearchLearningCurve(modelName, train, test, paramGrid, numThreads=12,\n",
    "                            k=5, showProgress=True, epochs=range(2, 10, 2),\n",
    "                            LCfile='../LearningCurves.txt'):\n",
    "    \"\"\"\n",
    "    \"Inspired\" (stolen) from sklearn gridsearch\n",
    "    https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_search.py\n",
    "    \"\"\"\n",
    "\n",
    "    curves = []\n",
    "    keys, values = zip(*paramGrid.items())\n",
    "\n",
    "    with open(LCfile, 'w') as outFile:\n",
    "        for val in itertools.product(*values):\n",
    "            params = dict(zip(keys, val))\n",
    "            thisModel = getModel(modelName, volubility=2)\n",
    "            outFile.write(str(type(thisModel)) + \"\\n\")\n",
    "            outFile.flush()\n",
    "\n",
    "            printLine = []\n",
    "            for key, value in params.items():\n",
    "                setattr(thisModel, key, value)\n",
    "                printLine.append((key, value))\n",
    "\n",
    "            outFile.write(' | '.join(f'{key}: {value}' for (key, value) in printLine) + \"\\n\")\n",
    "            outFile.flush()\n",
    "\n",
    "            _, pAtK, MAPatK, NDCGatK, AUCatK = \\\n",
    "                learningCurve(thisModel, train, test, epochs, k=k, outFile=outFile,\n",
    "                              showProgress=showProgress, numThreads=numThreads)\n",
    "\n",
    "            curves.append({'params': params,\n",
    "                           f'p@{k}': pAtK, f'MAP@{k}': MAPatK, f'NDCG@{k}': NDCGatK, f'AUC@{k}': AUCatK})\n",
    "            del thisModel\n",
    "\n",
    "    return curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "### Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:15:14.168491Z",
     "start_time": "2020-04-14T19:15:13.767877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting dataset lastfm\n",
      "type(artists): <class 'numpy.ndarray'>\n",
      "type(users): <class 'numpy.ndarray'>\n",
      "type(plays): <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(292385,) (358868,) (292385, 358868)\n"
     ]
    }
   ],
   "source": [
    "dataset = 'lastfm'\n",
    "artists, users, plays = fetchDataset(dataset, volubility=2)\n",
    "\n",
    "print(artists.shape, users.shape, plays.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test spit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:15:38.271851Z",
     "start_time": "2020-04-14T19:15:37.119201Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(plays, train_percentage=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide parameter search, @5 metrics\n",
    "\n",
    "#### Set parameter ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T05:58:54.159801Z",
     "start_time": "2020-04-13T05:58:54.153167Z"
    }
   },
   "outputs": [],
   "source": [
    "myFactors = [int(round(f)) for f in np.logspace(np.log10(20), 2, 7)]\n",
    "myλ = np.logspace(-4, 2, 7)\n",
    "myα = np.logspace(np.log10(5), np.log10(200), 6)\n",
    "print(myFactors)\n",
    "print(myλ)\n",
    "print(myα)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T05:58:54.382831Z",
     "start_time": "2020-04-13T05:58:54.380419Z"
    }
   },
   "outputs": [],
   "source": [
    "paramGrid = {'factors': myFactors,\n",
    "             'regularization': myλ,\n",
    "             'alpha': myα}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T05:58:54.489381Z",
     "start_time": "2020-04-13T05:58:54.486700Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modelName = 'als'\n",
    "myEpochs = range(5, 25)\n",
    "myLCfile = '../LearningCurvesLast.fm.0.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T22:33:12.166797Z",
     "start_time": "2020-04-13T05:58:55.710321Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "curves = gridSearchLearningCurve(modelName, train, test, paramGrid, numThreads=0,\n",
    "                                 showProgress=False, k=5, epochs=myEpochs, LCfile=myLCfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:32:41.787143Z",
     "start_time": "2020-04-14T00:32:41.783814Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(curves[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Massage list of dicts to get useful DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:32:46.635310Z",
     "start_time": "2020-04-14T00:32:46.630297Z"
    }
   },
   "outputs": [],
   "source": [
    "thang = [curves[x]['params'] for x in range(len(curves))]\n",
    "df0 = pd.DataFrame(thang)\n",
    "# df0.head()\n",
    "# df0.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:32:48.153910Z",
     "start_time": "2020-04-14T00:32:48.147614Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "blah = [curves[x]['p@k'] for x in range(len(curves))]\n",
    "df1 = pd.DataFrame(blah)\n",
    "# df1.head()\n",
    "# df1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:32:48.958745Z",
     "start_time": "2020-04-14T00:32:48.952434Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "blah = [curves[x]['MAP@k'] for x in range(len(curves))]\n",
    "df2 = pd.DataFrame(blah)\n",
    "# df2.head()\n",
    "# df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:32:50.905676Z",
     "start_time": "2020-04-14T00:32:50.899288Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "blah = [curves[x]['NDCG@k'] for x in range(len(curves))]\n",
    "df3 = pd.DataFrame(blah)\n",
    "# df3.head()\n",
    "# df3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:32:50.905676Z",
     "start_time": "2020-04-14T00:32:50.899288Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "blah = [curves[x]['AUC@k'] for x in range(len(curves))]\n",
    "df4 = pd.DataFrame(blah)\n",
    "# df4.head()\n",
    "# df4.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:32:53.082776Z",
     "start_time": "2020-04-14T00:32:53.049012Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df0, df1, df2, df3,df4], axis=1)\n",
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:33:24.017333Z",
     "start_time": "2020-04-14T00:33:23.975875Z"
    }
   },
   "outputs": [],
   "source": [
    "df.set_index(['factors', 'regularization', 'alpha'], inplace=True)\n",
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:34:41.606575Z",
     "start_time": "2020-04-14T00:34:41.554886Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics = ['p@k', 'MAP@k', 'NDCG@k', 'AUC@k']\n",
    "df.columns = pd.MultiIndex.from_product([metrics, myEpochs])# , names=['metric', 'epoch'])\n",
    "df.head(8)\n",
    "df.tail(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each count of epochs, find best parameters for each metric\n",
    "\n",
    "##### Find best `p@5` for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:45:28.580678Z",
     "start_time": "2020-04-14T00:45:28.564090Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indices = df.index\n",
    "cmaxs = df['p@k'].max()\n",
    "print(\"     epoch  factors\\t      λ\\t       α\\tind\\t    p@k\")\n",
    "for e in myEpochs:\n",
    "    ind = np.argmax(df[('p@k', e)] == cmaxs[e])\n",
    "    (factors, regularization, alpha) = indices[ind]\n",
    "    print(f\"\\t{e:2d}\\t{factors:3d}\\t{regularization:7.3f}\\t {alpha:7.3f}\\t{ind:3d}\\t{cmaxs[e]:7.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find best `MAP@5` for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:45:03.184878Z",
     "start_time": "2020-04-14T00:45:03.168789Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmaxs = df['MAP@k'].max()\n",
    "print(\"     epoch  factors\\t      λ\\t       α\\tind\\t  MAP@k\")\n",
    "for e in myEpochs:\n",
    "    ind = np.argmax(df[('MAP@k', e)] == cmaxs[e])\n",
    "    (factors, regularization, alpha) = indices[ind]\n",
    "    print(f\"\\t{e:2d}\\t{factors:3d}\\t{regularization:7.3f}\\t {alpha:7.3f}\\t{ind:3d}\\t{cmaxs[e]:7.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find best `NDCG@5` for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:46:19.893350Z",
     "start_time": "2020-04-14T00:46:19.876183Z"
    }
   },
   "outputs": [],
   "source": [
    "cmaxs = df['NDCG@k'].max()\n",
    "print(\"     epoch  factors\\t      λ\\t       α\\tind\\t NDCG@k\")\n",
    "for e in myEpochs:\n",
    "    ind = np.argmax(df[('NDCG@k', e)] == cmaxs[e])\n",
    "    (factors, regularization, alpha) = indices[ind]\n",
    "    print(f\"\\t{e:2d}\\t{factors:3d}\\t{regularization:7.3f}\\t {alpha:7.3f}\\t{ind:3d}\\t{cmaxs[e]:7.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find best `AUC@5` for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:46:51.940189Z",
     "start_time": "2020-04-14T00:46:51.923285Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmaxs = df['AUC@k'].max()\n",
    "print(\"     epoch  factors\\t      λ\\t       α\\tind\\t  AUC@k\")\n",
    "for e in myEpochs:\n",
    "    ind = np.argmax(df[('AUC@k', e)] == cmaxs[e])\n",
    "    (factors, regularization, alpha) = indices[ind]\n",
    "    print(f\"\\t{e:2d}\\t{factors:3d}\\t{regularization:7.3f}\\t {alpha:7.3f}\\t{ind:3d}\\t{cmaxs[e]:7.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrower grid search, `@20` metrics\n",
    "\n",
    "* Most of the better results were obtained for epochs > 15, so this time we will use myEpochs = range(16, 36, 2), skipping every other to speed up computation\n",
    "* Hints in papers suggest that larger latent feature counts may improve performance, so will extend range\n",
    "* Most of `@5` metrics above peaked with α = 200, so extend that range also\n",
    "* In nearly all instance, regularization values λ ≥ 10 worked best, so let that be a guide ...\n",
    "\n",
    "#### Set parameter ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T18:56:13.682223Z",
     "start_time": "2020-04-14T18:56:13.675160Z"
    }
   },
   "outputs": [],
   "source": [
    "myFactors = [int(round(f)) for f in np.logspace(np.log10(40), np.log10(300), 6)]\n",
    "myλ = np.logspace(0, np.log10(250), 5)\n",
    "myα = np.logspace(np.log10(20), np.log10(300), 5)\n",
    "print(myFactors)\n",
    "print(myλ)\n",
    "print(myα)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T18:59:37.025464Z",
     "start_time": "2020-04-14T18:59:37.022627Z"
    }
   },
   "outputs": [],
   "source": [
    "paramGrid = {'factors': myFactors,\n",
    "             'regularization': myλ,\n",
    "             'alpha': myα}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T18:59:39.076269Z",
     "start_time": "2020-04-14T18:59:39.073515Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modelName = 'als'\n",
    "myEpochs = range(15, 36, 2)\n",
    "myLCfile = '../LearningCurvesLast.fm.1.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:05:37.382266Z",
     "start_time": "2020-04-14T19:05:22.111192Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "curves = gridSearchLearningCurve(modelName, train, test, paramGrid, numThreads=0,\n",
    "                                 showProgress=False, k=20, epochs=myEpochs, LCfile=myLCfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:32:41.787143Z",
     "start_time": "2020-04-14T00:32:41.783814Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(curves[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Massage list of dicts to get useful DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:32:46.635310Z",
     "start_time": "2020-04-14T00:32:46.630297Z"
    }
   },
   "outputs": [],
   "source": [
    "thang = [curves[x]['params'] for x in range(len(curves))]\n",
    "df0 = pd.DataFrame(thang)\n",
    "# df0.head()\n",
    "# df0.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:32:48.153910Z",
     "start_time": "2020-04-14T00:32:48.147614Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "blah = [curves[x]['p@20'] for x in range(len(curves))]\n",
    "df1 = pd.DataFrame(blah)\n",
    "# df1.head()\n",
    "# df1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:32:48.958745Z",
     "start_time": "2020-04-14T00:32:48.952434Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "blah = [curves[x]['MAP@20'] for x in range(len(curves))]\n",
    "df2 = pd.DataFrame(blah)\n",
    "# df2.head()\n",
    "# df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:32:50.905676Z",
     "start_time": "2020-04-14T00:32:50.899288Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "blah = [curves[x]['NDCG@20'] for x in range(len(curves))]\n",
    "df3 = pd.DataFrame(blah)\n",
    "# df3.head()\n",
    "# df3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:32:50.905676Z",
     "start_time": "2020-04-14T00:32:50.899288Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "blah = [curves[x]['AUC@20'] for x in range(len(curves))]\n",
    "df4 = pd.DataFrame(blah)\n",
    "# df4.head()\n",
    "# df4.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:32:53.082776Z",
     "start_time": "2020-04-14T00:32:53.049012Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df0, df1, df2, df3,df4], axis=1)\n",
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:33:24.017333Z",
     "start_time": "2020-04-14T00:33:23.975875Z"
    }
   },
   "outputs": [],
   "source": [
    "df.set_index(['factors', 'regularization', 'alpha'], inplace=True)\n",
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:34:41.606575Z",
     "start_time": "2020-04-14T00:34:41.554886Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics = ['p@20', 'MAP@20', 'NDCG@20', 'AUC@20']\n",
    "df.columns = pd.MultiIndex.from_product([metrics, myEpochs])# , names=['metric', 'epoch'])\n",
    "df.head(8)\n",
    "df.tail(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each count of epochs, find best parameters for each metric\n",
    "\n",
    "##### Find best `p@20` for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:03:19.069401Z",
     "start_time": "2020-04-14T19:03:19.025323Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indices = df.index\n",
    "cmaxs = df['p@20'].max()\n",
    "print(\"     epoch  factors\\t      λ\\t       α\\tind\\t    p@20\")\n",
    "for e in myEpochs:\n",
    "    ind = np.argmax(df[('p@20', e)] == cmaxs[e])\n",
    "    (factors, regularization, alpha) = indices[ind]\n",
    "    print(f\"\\t{e:2d}\\t{factors:3d}\\t{regularization:7.3f}\\t {alpha:7.3f}\\t{ind:3d}\\t{cmaxs[e]:7.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find best `MAP@20` for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:45:03.184878Z",
     "start_time": "2020-04-14T00:45:03.168789Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmaxs = df['MAP@20'].max()\n",
    "print(\"     epoch  factors\\t      λ\\t       α\\tind\\t  MAP@20\")\n",
    "for e in myEpochs:\n",
    "    ind = np.argmax(df[('MAP@20', e)] == cmaxs[e])\n",
    "    (factors, regularization, alpha) = indices[ind]\n",
    "    print(f\"\\t{e:2d}\\t{factors:3d}\\t{regularization:7.3f}\\t {alpha:7.3f}\\t{ind:3d}\\t{cmaxs[e]:7.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find best `NDCG@20` for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:46:19.893350Z",
     "start_time": "2020-04-14T00:46:19.876183Z"
    }
   },
   "outputs": [],
   "source": [
    "cmaxs = df['NDCG@20'].max()\n",
    "print(\"     epoch  factors\\t      λ\\t       α\\tind\\t NDCG@20\")\n",
    "for e in myEpochs:\n",
    "    ind = np.argmax(df[('NDCG@20', e)] == cmaxs[e])\n",
    "    (factors, regularization, alpha) = indices[ind]\n",
    "    print(f\"\\t{e:2d}\\t{factors:3d}\\t{regularization:7.3f}\\t {alpha:7.3f}\\t{ind:3d}\\t{cmaxs[e]:7.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find best `AUC@20` for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T00:46:51.940189Z",
     "start_time": "2020-04-14T00:46:51.923285Z"
    }
   },
   "outputs": [],
   "source": [
    "cmaxs = df['AUC@20'].max()\n",
    "print(\"     epoch  factors\\t      λ\\t       α\\tind\\t  AUC@20\")\n",
    "for e in myEpochs:\n",
    "    ind = np.argmax(df[('AUC@20', e)] == cmaxs[e])\n",
    "    (factors, regularization, alpha) = indices[ind]\n",
    "    print(f\"\\t{e:2d}\\t{factors:3d}\\t{regularization:7.3f}\\t {alpha:7.3f}\\t{ind:3d}\\t{cmaxs[e]:7.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
