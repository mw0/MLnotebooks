{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Similar Music Using `Implicit` and Alternating Least Squares (ALS) for Matrix Factorization\n",
    "\n",
    "This follow's Ethan Rosenthal's [Intro to Implicit Matrix Factorization: Classic ALS with Sketchfab Models](https://www.ethanrosenthal.com/2016/10/19/implicit-mf-part-1]).\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "**Next two lines are for pretty output for all prints in a Pandas cell, not just the last.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T08:07:10.779038Z",
     "start_time": "2020-04-10T08:07:10.775847Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T08:07:11.219687Z",
     "start_time": "2020-04-10T08:07:10.879369Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "import pickle\n",
    "import csv\n",
    "# import implicit\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.approximate_als import (AnnoyAlternatingLeastSquares,\n",
    "                                      FaissAlternatingLeastSquares,\n",
    "                                      NMSLibAlternatingLeastSquares)\n",
    "from implicit.bpr import BayesianPersonalizedRanking\n",
    "from implicit.datasets.lastfm import get_lastfm\n",
    "from implicit.datasets.movielens import get_movielens\n",
    "from implicit.datasets.reddit import get_reddit\n",
    "from implicit.datasets.sketchfab import get_sketchfab\n",
    "from implicit.datasets.million_song_dataset import get_msd_taste_profile\n",
    "from implicit.lmf import LogisticMatrixFactorization\n",
    "from implicit.nearest_neighbours import (BM25Recommender, CosineRecommender,\n",
    "                                         TFIDFRecommender, bm25_weight)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import itertools\n",
    "import copy\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T08:07:11.227206Z",
     "start_time": "2020-04-10T08:07:11.222139Z"
    }
   },
   "outputs": [],
   "source": [
    "# maps command line model argument to class name\n",
    "models = {\"als\":  AlternatingLeastSquares,\n",
    "          \"nmslib_als\": NMSLibAlternatingLeastSquares,\n",
    "          \"annoy_als\": AnnoyAlternatingLeastSquares,\n",
    "          \"faiss_als\": FaissAlternatingLeastSquares,\n",
    "          \"tfidf\": TFIDFRecommender,\n",
    "          \"cosine\": CosineRecommender,\n",
    "          \"bpr\": BayesianPersonalizedRanking,\n",
    "          \"lmf\": LogisticMatrixFactorization,\n",
    "          \"bm25\": BM25Recommender}\n",
    "\n",
    "dataSets = {\"lastfm\": get_lastfm,\n",
    "            \"movielens\": get_movielens,\n",
    "            \"reddit\": get_reddit,\n",
    "            \"sketchfab\": get_sketchfab,\n",
    "            \"million_song\": get_msd_taste_profile}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "### `getModel()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T08:07:11.237022Z",
     "start_time": "2020-04-10T08:07:11.229665Z"
    }
   },
   "outputs": [],
   "source": [
    "def getModel(modelName):\n",
    "    \"\"\"\n",
    "    This merely fetches a particular implicit model type\n",
    "    \"\"\"\n",
    "    print(\"getting model %s\" % modelName)\n",
    "    modelClass = models.get(modelName)\n",
    "    if not modelClass:\n",
    "        raise ValueError(\"Unknown Model '%s'\" % modelName)\n",
    "\n",
    "    # some default params\n",
    "    if issubclass(modelClass, AlternatingLeastSquares):\n",
    "        params = {'factors': 16, 'dtype': np.float32}\n",
    "    elif modelName == \"bm25\":\n",
    "        params = {'K1': 100, 'B': 0.5}\n",
    "    elif modelName == \"bpr\":\n",
    "        params = {'factors': 63}\n",
    "    elif modelName == \"lmf\":\n",
    "        params = {'factors': 30, \"iterations\": 40, \"regularization\": 1.5}\n",
    "    else:\n",
    "        params = {}\n",
    "\n",
    "    return modelClass(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `getDatums()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T08:07:11.245015Z",
     "start_time": "2020-04-10T08:07:11.238998Z"
    }
   },
   "outputs": [],
   "source": [
    "def getDatums(dataset):\n",
    "    \"\"\"\n",
    "    No real work to do here, since the heavy lifting is done by the implicit.dataSets module\n",
    "    \"\"\"\n",
    "    print(f\"getting dataset {dataset}\")\n",
    "    # artists, users, plays = get_lastfm()\n",
    "    getdata = dataSets.get(dataset)\n",
    "    if not getdata:\n",
    "        raise ValueError(f\"Unknown Model {dataset}\")\n",
    "    artists, users, plays = getdata()\n",
    "\n",
    "    return artists, users, plays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `thresholdLikes()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T08:07:11.279754Z",
     "start_time": "2020-04-10T08:07:11.266818Z"
    }
   },
   "outputs": [],
   "source": [
    "def thresholdLikes(df, UIDmin, MIDmin):\n",
    "    \"\"\"\n",
    "    Removes from data set those items having fewer than \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    userCt = df.uid.unique().shape[0]\n",
    "    itemCt = df.mid.unique().shape[0]\n",
    "    sparsity = float(df.shape[0]) / float(userCt*itemCt) * 100\n",
    "    print(\"Starting likes info\")\n",
    "    print(\"Number of users: {userCt}\")\n",
    "    print(\"Number of models: {itemCt}\")\n",
    "    print(\"Sparsity: {sparsity:4.3f}%\")\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        starting_shape = df.shape[0]\n",
    "        mid_counts = df.groupby('uid').mid.count()\n",
    "        df = df[~df.uid.isin(mid_counts[mid_counts < MIDmin].index.tolist())]\n",
    "        uid_counts = df.groupby('mid').uid.count()\n",
    "        df = df[~df.mid.isin(uid_counts[uid_counts < UIDmin].index.tolist())]\n",
    "        ending_shape = df.shape[0]\n",
    "        if starting_shape == ending_shape:\n",
    "            done = True\n",
    "    \n",
    "    assert(df.groupby('uid').mid.count().min() >= MIDmin)\n",
    "    assert(df.groupby('mid').uid.count().min() >= UIDmin)\n",
    "    \n",
    "    userCt = df.uid.unique().shape[0]\n",
    "    itemCt = df.mid.unique().shape[0]\n",
    "    sparsity = float(df.shape[0]) / float(userCt*itemCt) * 100\n",
    "    print('Ending likes info')\n",
    "    print('Number of users: {}'.format(userCt))\n",
    "    print('Number of models: {}'.format(itemCt))\n",
    "    print('Sparsity: {:4.3f}%'.format(sparsity))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `trainTestSplit()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T08:23:14.810570Z",
     "start_time": "2020-04-10T08:23:14.800495Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainTestSplit(ratings, splitCount, fraction=None):\n",
    "    \"\"\"\n",
    "    Split recommendation data into train and test sets\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    ratings : scipy.sparse matrix\n",
    "        Interactions between users and items.\n",
    "    splitCount : int\n",
    "        Number of user-item-interactions per user to move\n",
    "        from training to test set.\n",
    "    fractions : float\n",
    "        Fraction of users to split off some of their\n",
    "        interactions into test set. If None, then all \n",
    "        users are considered.\n",
    "    \"\"\"\n",
    "\n",
    "    # Note: likely not the fastest way to do things below.\n",
    "    train = ratings.copy().tocoo()\n",
    "    test = sparse.lil_matrix(train.shape)\n",
    "    \n",
    "    if fraction:\n",
    "        try:\n",
    "            userIndex = np.random.choice(\n",
    "                np.where(np.bincount(train.row) >= splitCount * 2)[0], \n",
    "                replace=False,\n",
    "                size=np.int32(np.floor(fraction * train.shape[0]))\n",
    "            ).tolist()\n",
    "        except:\n",
    "            raise Exception(f\"Not enough users with > {2*splitCount} \"\n",
    "                            f\"interactions for fraction of {fraction}\")\n",
    "    else:\n",
    "        userIndex = range(train.shape[0])\n",
    "        \n",
    "    train = train.tolil()\n",
    "\n",
    "    for user in userIndex:\n",
    "        testRatings = np.random.choice(ratings.getrow(user).indices, \n",
    "                                        size=splitCount, \n",
    "                                        replace=False)\n",
    "        train[user, testRatings] = 0.\n",
    "        # These are just 1.0 right now\n",
    "        test[user, testRatings] = ratings[user, testRatings]\n",
    "   \n",
    "    \n",
    "    # Test and training are truly disjoint\n",
    "    assert(train.multiply(test).nnz == 0)\n",
    "    return train.tocsr(), test.tocsr(), userIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `calculateSimilarArtists()`\n",
    "\n",
    "After fetching the data, applies the `similar_items` API of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T08:07:11.481168Z",
     "start_time": "2020-04-10T08:07:11.470157Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculateSimilarArtists(output_filename, dataset, model_name=\"als\"):\n",
    "    \"\"\"\n",
    "    Generates a list of similar artists in lastfm by utilizing the\n",
    "    'similar_items' api of the models\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the data\n",
    "    artists, users, plays = getDatums(dataset)\n",
    "\n",
    "    # create a model from the input data\n",
    "    model = get_model(model_name)\n",
    "\n",
    "    # if we're training an ALS based model, weight input for last.fm\n",
    "    # by bm25\n",
    "    if issubclass(model.__class__, AlternatingLeastSquares):\n",
    "        # lets weight these models by bm25weight.\n",
    "        logging.debug(\"weighting matrix by bm25_weight\")\n",
    "        plays = bm25_weight(plays, K1=100, B=0.8)\n",
    "\n",
    "        # also disable building approximate recommend index\n",
    "        model.approximate_recommend = False\n",
    "\n",
    "    # this is actually disturbingly expensive:\n",
    "    plays = plays.tocsr()\n",
    "\n",
    "    logging.debug(\"training model %s\", model_name)\n",
    "    start = time.time()\n",
    "    model.fit(plays)\n",
    "    logging.debug(\"trained model '%s' in %0.2fs\", model_name,\n",
    "                  time.time() - start)\n",
    "\n",
    "    # write out similar artists by popularity\n",
    "    start = time.time()\n",
    "    logging.debug(\"calculating top artists\")\n",
    "\n",
    "    user_count = np.ediff1d(plays.indptr)\n",
    "    to_generate = sorted(np.arange(len(artists)), key=lambda x: -user_count[x])\n",
    "\n",
    "    # write out as a TSV of artistid, otherartistid, score\n",
    "    logging.debug(\"writing similar items\")\n",
    "    with tqdm.tqdm(total=len(to_generate)) as progress:\n",
    "        with codecs.open(output_filename, \"w\", \"utf8\") as o:\n",
    "            for artistid in to_generate:\n",
    "                artist = artists[artistid]\n",
    "                for other, score in model.similar_items(artistid, 11):\n",
    "                    o.write(\"%s\\t%s\\t%s\\n\" % (artist, artists[other], score))\n",
    "                progress.update(1)\n",
    "\n",
    "    logging.debug(\"generated similar artists in %0.2fs\",  time.time() - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `calculateRecommendations()`\n",
    "\n",
    "Generates artist recommendations for each user in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T08:07:11.536963Z",
     "start_time": "2020-04-10T08:07:11.526990Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculateRecommendations(output_filename, model_name=\"als\"):\n",
    "    \"\"\" Generates artist recommendations for each user in the dataset \"\"\"\n",
    "\n",
    "    # Get the data\n",
    "    artists, users, plays = getDatums(dataset)\n",
    "\n",
    "    # create a model from the input data\n",
    "    model = get_model(model_name)\n",
    "\n",
    "    # train the model based off input params\n",
    "\n",
    "    # if we're training an ALS based model, weight input for last.fm\n",
    "    # by bm25\n",
    "    if issubclass(model.__class__, AlternatingLeastSquares):\n",
    "        # lets weight these models by bm25weight.\n",
    "        logging.debug(\"weighting matrix by bm25_weight\")\n",
    "        plays = bm25_weight(plays, K1=100, B=0.8)\n",
    "\n",
    "        # also disable building approximate recommend index\n",
    "        model.approximate_similar_items = False\n",
    "\n",
    "    # this is actually disturbingly expensive:\n",
    "    plays = plays.tocsr()\n",
    "\n",
    "    logging.debug(\"training model %s\", model_name)\n",
    "    start = time.time()\n",
    "    model.fit(plays)\n",
    "    logging.debug(\"trained model '%s' in %0.2fs\", model_name, time.time() - start)\n",
    "\n",
    "    # generate recommendations for each user and write out to a file\n",
    "    start = time.time()\n",
    "    user_plays = plays.T.tocsr()\n",
    "    with tqdm.tqdm(total=len(users)) as progress:\n",
    "        with codecs.open(output_filename, \"w\", \"utf8\") as o:\n",
    "            for userid, username in enumerate(users):\n",
    "                for artistid, score in model.recommend(userid, user_plays):\n",
    "                    o.write(\"%s\\t%s\\t%s\\n\" % (username, artists[artistid],\n",
    "                                              score))\n",
    "                progress.update(1)\n",
    "    logging.debug(\"generated recommendations in %0.2fs\",  time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `calculateMSE()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T08:07:11.636157Z",
     "start_time": "2020-04-10T08:07:11.630922Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculateMSE(model, ratings, userIndex=None):\n",
    "    preds = model.predict_for_customers()\n",
    "    if userIndex:\n",
    "        return mean_squared_error(ratings[userIndex, :].toarray().ravel(),\n",
    "                                  preds[user_index, :].ravel())\n",
    "\n",
    "    return mean_squared_error(ratings.toarray().ravel(),\n",
    "                              preds.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `precisionAtK()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T08:07:11.752795Z",
     "start_time": "2020-04-10T08:07:11.746028Z"
    }
   },
   "outputs": [],
   "source": [
    "def precisionAtK(model, ratings, k=5, userIndex=None):\n",
    "\n",
    "    if not userIndex:\n",
    "        userIndex = range(ratings.shape[0])\n",
    "    ratings = ratings.tocsr()\n",
    "    precisions = []\n",
    "\n",
    "    # Note: line below may become infeasible for large datasets.\n",
    "    predictions = model.predict_for_customers()\n",
    "\n",
    "    for user in userIndex:\n",
    "        # In case of large dataset, compute predictions row-by-row like below\n",
    "        # predictions = np.array([model.predict(row, i) for i in xrange(ratings.shape[1])])\n",
    "        topK = np.argsort(-predictions[user, :])[:k]\n",
    "        labels = ratings.getrow(user).indices\n",
    "        precision = float(len(set(topK) & set(labels))) / float(k)\n",
    "        precisions.append(precision)\n",
    "\n",
    "    return np.mean(precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `printLog()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T08:07:11.816428Z",
     "start_time": "2020-04-10T08:07:11.808350Z"
    }
   },
   "outputs": [],
   "source": [
    "def printLog(row, header=False, spacing=12):\n",
    "    top = ''\n",
    "    middle = ''\n",
    "    bottom = ''\n",
    "    for r in row:\n",
    "        top += '+{}'.format('-'*spacing)\n",
    "        if isinstance(r, str):\n",
    "            middle += '| {0:^{1}} '.format(r, spacing-2)\n",
    "        elif isinstance(r, int):\n",
    "            middle += '| {0:^{1}} '.format(r, spacing-2)\n",
    "        elif isinstance(r, float):\n",
    "            middle += '| {0:^{1}.5f} '.format(r, spacing-2)\n",
    "        bottom += '+{}'.format('='*spacing)\n",
    "    top += '+'\n",
    "    middle += '|'\n",
    "    bottom += '+'\n",
    "    if header:\n",
    "        print(top)\n",
    "        print(middle)\n",
    "        print(bottom)\n",
    "    else:\n",
    "        print(middle)\n",
    "        print(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `learningCurve()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T08:43:03.069118Z",
     "start_time": "2020-04-10T08:43:03.059045Z"
    }
   },
   "outputs": [],
   "source": [
    "def learningCurve(model, train, test, epochs, k=5, userIndex=None):\n",
    "    if not userIndex:\n",
    "        userIndex = range(train.shape[0])\n",
    "    previousEpoch = 0\n",
    "    trainPrecision = []\n",
    "    trainMSE = []\n",
    "    testPrecision = []\n",
    "    testMSE = []\n",
    "    \n",
    "    headers = ['epochs', 'p@k train', 'p@k test',\n",
    "               'mse train', 'mse test']\n",
    "    printLog(headers, header=True)\n",
    "    \n",
    "    for epoch in epochs:\n",
    "        model.iterations = epoch - previousEpoch\n",
    "        if not hasattr(model, 'user_vectors'):\n",
    "            model.fit(train)\n",
    "        else:\n",
    "            model.fit_partial(train)\n",
    "        trainMSE.append(calculateMSE(model, train, userIndex))\n",
    "        trainPrecision.append(precisionAtK(model, train, k, userIndex))\n",
    "        testMSE.append(calculateMSE(model, test, userIndex))\n",
    "        testPrecision.append(precisionAtK(model, test, k, userIndex))\n",
    "        row = [epoch, trainPrecision[-1], testPrecision[-1],\n",
    "               trainMSE[-1], testMSE[-1]]\n",
    "        printLog(row)\n",
    "        previousEpoch = epoch\n",
    "    return model, trainPrecision, trainMSE, testPrecision, testMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `gridSearchLearningCurve()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T08:35:41.320991Z",
     "start_time": "2020-04-10T08:35:41.311505Z"
    }
   },
   "outputs": [],
   "source": [
    "def gridSearchLearningCurve(baseModel, train, test, paramGrid,\n",
    "                            userIndex=None, patk=5, epochs=range(2, 40, 2)):\n",
    "    \"\"\"\n",
    "    \"Inspired\" (stolen) from sklearn gridsearch\n",
    "    https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_search.py\n",
    "    \"\"\"\n",
    "    curves = []\n",
    "    keys, values = zip(*paramGrid.items())\n",
    "    for v in itertools.product(*values):\n",
    "        params = dict(zip(keys, v))\n",
    "        thisModel = copy.deepcopy(baseModel)\n",
    "        print_line = []\n",
    "        for k, v in params.items():\n",
    "            setattr(thisModel, k, v)\n",
    "            print_line.append((k, v))\n",
    "\n",
    "        print(' | '.join('{}: {}'.format(k, v) for (k, v) in print_line))\n",
    "        _, patkTrain, MSEtrain, patkTest, MSEtest = learningCurve(thisModel, train, test,\n",
    "                                                                  epochs, k=patk,\n",
    "                                                                  userIndex=userIndex)\n",
    "        curves.append({'params': params,\n",
    "                       'patk': {'train': patkTrain, 'test': patkTest},\n",
    "                       'MSE': {'train': MSEtrain, 'test': MSEtest}})\n",
    "    return curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T08:11:20.182845Z",
     "start_time": "2020-04-10T08:11:19.790368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting dataset lastfm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray,\n",
       " array([' 2 ', ' 58725ab=>', ' 80lİ yillarin tÜrkÇe sÖzlÜ aŞk Şarkilari',\n",
       "        ' amy winehouse'], dtype=object))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray,\n",
       " array(['00000c289a1829a808ac09c00daf10bc3c4e223b',\n",
       "        '00001411dc427966b17297bf4d69e7e193135d89',\n",
       "        '00004d2ac9316e22dc007ab2243d6fcb239e707d',\n",
       "        '000063d3fe1cf2ba248b9e3c3f0334845a27a6bf'], dtype=object))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(scipy.sparse.csr.csr_matrix,\n",
       " <4x358868 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 7 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists, users, plays = getDatums('lastfm')\n",
    "\n",
    "type(artists), artists[:4]\n",
    "type(users), users[:4]\n",
    "type(plays), plays[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T08:10:08.343396Z",
     "start_time": "2020-04-10T08:10:08.338295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((292385,), (358868,), (292385, 358868))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists.shape, users.shape, plays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T08:15:16.824503Z",
     "start_time": "2020-04-10T08:15:16.776387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01671209636692248"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*plays.count_nonzero()/(artists.shape[0]*users.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T08:24:33.599466Z",
     "start_time": "2020-04-10T08:23:22.990467Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test, userIndex = trainTestSplit(plays, 5, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T08:29:27.301789Z",
     "start_time": "2020-04-10T08:29:27.296519Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:implicit:GPU training requires factor size to be a multiple of 32. Increasing factors from 16 to 32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting model als\n"
     ]
    }
   ],
   "source": [
    "paramGrid = {'num_factors': [20, 40, 80],\n",
    "              'regularization': [1e-3, 1e-1, 1e1],\n",
    "              'alpha': [32, 64, 128]}\n",
    "\n",
    "modelALS = getModel('als')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T08:37:21.678400Z",
     "start_time": "2020-04-10T08:37:19.612288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_factors: 20 | regularization: 0.001 | alpha: 32\n",
      "+------------+------------+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  | mse train  |  mse test  |\n",
      "+============+============+============+============+============+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5f19cc1b87476b8ad2a3f03f0478b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AlternatingLeastSquares' object has no attribute 'predict_for_customers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-8b17885638ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                                  \u001b[0mparamGrid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                  \u001b[0muserIndex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muserIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                  patk=5)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-b0b7548f20de>\u001b[0m in \u001b[0;36mgridSearchLearningCurve\u001b[0;34m(baseModel, train, test, paramGrid, userIndex, patk, epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m         _, patkTrain, MSEtrain, patkTest, MSEtest = learningCurve(thisModel, train, test,\n\u001b[1;32m     19\u001b[0m                                                                   \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                                                                   userIndex=userIndex)\n\u001b[0m\u001b[1;32m     21\u001b[0m         curves.append({'params': params,\n\u001b[1;32m     22\u001b[0m                        \u001b[0;34m'patk'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpatkTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpatkTest\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-5b1a1bb2e65c>\u001b[0m in \u001b[0;36mlearningCurve\u001b[0;34m(model, train, test, epochs, k, userIndex)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtrainMSE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculateMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mtrainPrecision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecisionAtK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtestMSE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculateMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-f0c7e0896c35>\u001b[0m in \u001b[0;36mcalculateMSE\u001b[0;34m(model, ratings, userIndex)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculateMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserIndex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_for_customers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muserIndex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         return mean_squared_error(ratings[userIndex, :].toarray().ravel(),\n\u001b[1;32m      5\u001b[0m                                   preds[user_index, :].ravel())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AlternatingLeastSquares' object has no attribute 'predict_for_customers'"
     ]
    }
   ],
   "source": [
    "curves = gridSearchLearningCurve(modelALS, train, test,\n",
    "                                 paramGrid,\n",
    "                                 userIndex=userIndex,\n",
    "                                 patk=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
