{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to CORD-19 dataset - NLP and Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# NLP Text processing libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Tokenizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Unsupervised learning\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note - the first time you run this notebook, you need to install data from some of the NLP packages\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a huge number of NLP packages that you can use - here we will just use NLTK, as it is realtively simple to understand and use, but some other common packages (which you should totally check out) include:\n",
    "- Spacy\n",
    "- Gensim\n",
    "- SparkNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"data/CORD-19-research-challenge/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the metadata - find a suitable subset of files\n",
    "\n",
    "This is going to be vital for loading appropriate data, and understanding what we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the metadata csv\n",
    "metadata = pd.read_csv(os.path.join(datadir, \"metadata.csv\"))\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see all of the possible sources of the papers\n",
    "metadata[\"source_x\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get a list of all the papers that are in the BioRxiv\n",
    "bioarxiv_locations = metadata[\"source_x\"] == \"BioRxiv\"\n",
    "bioarxiv_df = metadata.loc[bioarxiv_locations]\n",
    "bioarxiv_papers = bioarxiv_df[\"pdf_json_files\"].dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many papers do we have?\n",
    "print(f\"There are {len(bioarxiv_papers)} papers in our sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the papers into a dataframe so we can get started\n",
    "\n",
    "Each of the papers exists as a seperate json file, so we now want to load those into memory. Note, many of these will be very large - the whole dataset is larger than pandas can efficiently take in a single data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the papers into a list of JSON objects, and then load into a dataframe\n",
    "json_extracts = [json.load(open(os.path.join(datadir, paper), \"rb\")) for paper in bioarxiv_papers]\n",
    "df = pd.DataFrame.from_records(json_extracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a loook at one of the abstracts\n",
    "df.iloc[0][\"abstract\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that some of the abstracts are empty - let's get rid of those\n",
    "df = df[df[\"abstract\"].map(lambda x: len(x)) > 0]\n",
    "df.reset_index(inplace=True)\n",
    "print(f\"New number of papers = {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's extract the abstracts\n",
    "df[\"abstract\"] = df[\"abstract\"].apply(lambda x: x[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the abstracts to move forward with\n",
    "df_nlp = df[[\"paper_id\", \"abstract\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing part 1: Peform some simple text filtering\n",
    "\n",
    "We want to remove elements of the text that are perhaps useful for idomatic language and human understanding, but are not necessarily needed in order to extract useful information from the documents\n",
    "\n",
    "This includes things like:\n",
    "- Punctuation\n",
    "- Capitalization\n",
    "- stop words\n",
    "\n",
    "In English, stop words include common linking words and articles such as:\n",
    "- a\n",
    "- an\n",
    "- the\n",
    "- but\n",
    "\n",
    "Most NLP packages contain a stopwords list, here we will use the one from the Naturual Language Tool Kit (NLTK)\n",
    "\n",
    "Note also - you can add to or create your own custom list of stopwords - for example you may wish to exclude the word \"covid\" from some analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove the punctuation from the corpus\n",
    "def remove_punctuation(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, \"\")\n",
    "    return text\n",
    "\n",
    "df_nlp[\"abstract\"] = df_nlp[\"abstract\"].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all capitalizations in our corpus\n",
    "def make_lower_case(text):\n",
    "    lower_words_list = [word.lower() for word in text.split(\" \")] # note - returns a list of words\n",
    "    return \" \".join(lower_words_list)\n",
    "    \n",
    "df_nlp[\"abstract\"] = df_nlp[\"abstract\"].apply(make_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the NLTK stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove any of these stopwords from our corpus\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_text = [word for word in text.split(\" \") if word not in stop_words] # note - returns a list of words\n",
    "    return \" \".join(filtered_text)\n",
    "    \n",
    "df_nlp[\"abstract\"] = df_nlp[\"abstract\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing part 2 - stemming and lemmatizing\n",
    "\n",
    "We often want to reduce the space of all possible words further by combining semantically similar information together. These could include things like:\n",
    "- Combining singular and plural words together\n",
    "- Combining synonyms together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "Stemming is the idea of removing certain common prefixes of words - for example removing the s at the end of words, with the hope of making them the same. Note - this can be too aggressive in some contexts\n",
    "\n",
    "Various stemming schemes exist, with varying rules and levels of aggression about what parts of words they will remove. Some examples within NLTK are\n",
    "- Snowball stemmer\n",
    "- Porter stemmer\n",
    "\n",
    "To give an example the sentence <strong>Programers program with programing languages</strong> becomes <strong>program program with program languag</strong> when passed through a porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(text):\n",
    "    \"\"\"\n",
    "    Perform stemming on a single element of the corpus\n",
    "    \"\"\"\n",
    "    porter = PorterStemmer()  # Define the PorterStemmer\n",
    "    filtered_text = [porter.stem(word) for word in text.split(\" \")] # note - returns a list of words\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "df_nlp[\"abstract\"] = df_nlp[\"abstract\"].apply(stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing\n",
    "\n",
    "The idea of lemmatizing is to group together words that are variants of the same word, such as synonyms. It can be used with, or instead or, stemming.\n",
    "\n",
    "While fill morphological analysis could make something more human readable, it doesn't guarantee any practical benefit over stemming for information retrieval purposes.\n",
    "\n",
    "There exist a number of models for performing lemmatization, one of the most famous being WordNet, which we will use here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(text):\n",
    "    \"\"\"\n",
    "    Perform lemmatizing on a single element of the corpus\n",
    "    \"\"\"\n",
    "    wordnet = WordNetLemmatizer()  # Define the PorterStemmer\n",
    "    filtered_text = [wordnet.lemmatize(word) for word in text.split(\" \")] # note - returns a list of words\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "df_nlp[\"abstract\"] = df_nlp[\"abstract\"].apply(lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing part 3: Tokenizing\n",
    "\n",
    "So now out data is reasonably constructed, we want to begin to prepare the data for input to a machine learning model\n",
    "\n",
    "Recall - ML models are mathematical objects, so we need to create a mathematical model of our text data. There are many possible schemes, often dependent upon the technology that you intend to use.\n",
    "\n",
    "For this example, we will show a few possible schemes - the bag of words representation and the TF-IDF representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words / Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we convert every document to a matrix where each column is a word, each row is a document, and the value in each column is how many times the word appears in the document.\n",
    "\n",
    "For example, the phrase \"the cat sat on the mat\" becomes:\n",
    "\n",
    "| the | cat | sat | on | mat | dog | ran |\n",
    "|-----|-----|-----|----|-----|-----|-----|\n",
    "|  2  |  1  |  1  |  1 |  1  |  0  |  0  |\n",
    "\n",
    "Every distinct word that appears in the training set will have a unique column. This is called a <strong>vocabulary</strong>\n",
    "\n",
    "Note - only a small subset of words are likely to appear in a single document - most elements in a single row are likely to be zero. Thus, this matrix is <strong>sparse</strong> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "count_matrix = count_vect.fit_transform(df_nlp[\"abstract\"])\n",
    "print(count_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a sparse matrix, so cannot trivially be printed out\n",
    "print(count_matrix[0,1:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But we can print out the words if we want to:\n",
    "word_list = count_vect.get_feature_names()\n",
    "test_word_counts = count_matrix[0, 1:2000]\n",
    "for n in count_matrix[0, 1:2000].indices:\n",
    "    print(f\"the word '{word_list[n]}' appears {test_word_counts[0, n]} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term-Frequency Inverse-Document-Frequency (TFIDF) Vectorizer\n",
    "\n",
    "This technique is super useful for information retrieval and search, and is regularly used as a preprocessing step across a range of NLP modelling approaches.\n",
    "\n",
    "The idea behind it is that certain words are more important in telling documents apart than others. For example, the word \"football\" commonly occurs in sports articles, but will be rare in other kinds of article. TFIDF tries to capitalize on this by creating weights that are high when a word appears a lot, but only in a few documents, and low when a word is infrequent or appears in a lot of documents\n",
    "\n",
    "The score for each word is:\n",
    "\n",
    "<strong>number of times word appears in the document</strong> / <strong>number of documents the word appears in at least once</strong>\n",
    "\n",
    "So if the word \"cat\" appears twice in a document, and is present at least once in 10 documents, the TFIDF value would be 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vect.fit_transform(df_nlp[\"abstract\"])\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is also a sparse matrix, so cannot trivially be printed out\n",
    "# Bear in mind - as the number of documents increases, the TFIDF score will tend to get smaller\n",
    "print(tfidf_matrix[0,1:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = count_vect.get_feature_names()\n",
    "test_word_counts = count_matrix[0, 1:2000]\n",
    "for n in count_matrix[0, 1:2000].indices:\n",
    "    print(f\"the word '{word_list[n]}' has a TFIDF score of {test_word_counts[0, n]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More tokenizing choices\n",
    "\n",
    "You aren't limited to just unigrams (single words). You could also do bigrams (each token is two consecutive words), trigrams etc\n",
    "\n",
    "You can also do combinations\n",
    "\n",
    "And there are other options like skipgrams, which is used by the well known word2vec embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_bg = TfidfVectorizer(ngram_range = (1,2))\n",
    "tfidf_matrix_bg = tfidf_vect_bg.fit_transform(df_nlp[\"abstract\"])\n",
    "print(tfidf_matrix_bg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = tfidf_vect_bg.get_feature_names()\n",
    "test_word_counts = tfidf_matrix_bg[0, 1:8000]\n",
    "for n in test_word_counts.indices:\n",
    "    print(f\"the word '{word_list[n]}' has a TFIDF score of {test_word_counts[0, n]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remainder of this session, we will be working with the TFIDF vectorizer, with the unigram tokenizing scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with some unsupervised learning\n",
    "\n",
    "This dataset does not come with any labels. Now, we could go though and manually create labels, or we can try and gain some insight into the data with some unsupervised learning\n",
    "\n",
    "Unsupervised learning is a technique wherby we, the human, pass in some data to an algorithm with allow it to \"learn\" something about the data on its own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can we extract something about the topics that are contained in our corpus?\n",
    "\n",
    "Topic modelling can be done using a range of techniques. The idea here is to try and find \"topics\" that might exist in our corpus - ie clusters of \"similar\" papers, which we might interpret as different topics.\n",
    "\n",
    "Caveat - this is much easier to do on a corpus that has a broader range of different topics! \n",
    "\n",
    "There are a range of techniques that we can use to perform topic modelling, including:\n",
    "- Non-negative matrix factorization (NMF)\n",
    "- Latent Dirichlet Allocatoin (LDA)\n",
    "\n",
    "For the purposes of this exercise, we will look at the NMF algorithm, and see how it can be leveraged. Note though, a lot of the kaggle submissions are using LDA - so definitely have a go with that technique\n",
    "\n",
    "This is an example of a dimensionality reduction technique - we are trying to transform our data in such a way that valuable information is surfaced in early dimensions, enabling us to remove later dimensions. In the case below, we will attempt to transform the data such that it can be optimally reduced to 10 dimensions.\n",
    "\n",
    "Due to the way this algorithm works, the 10 dimensions can often be interpreted as topics that appear in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an NMF object (you're getting the pattern by now!)\n",
    "# Here, we will try and see if we can detect 10 different topics\n",
    "model = NMF(n_components = 10)\n",
    "features = model.fit_transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the list of words that could possibly exist\n",
    "words = np.array(tfidf_vect.get_feature_names())\n",
    "\n",
    "# Extract the h-matrix from the NMF algorithm\n",
    "h_sk = model.components_\n",
    "\n",
    "# Extract the top 18words from each of the observed 10 topics\n",
    "topics = np.flip(h_sk.argsort(axis = 1), axis=1)[:, :8]\n",
    "for n, topic in enumerate(topics):\n",
    "    print(f\"topic {n+1}: {words[topic]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this from a human angle, we can start to see possible topics emerging from this. Some examples:\n",
    "\n",
    "- Topics 1 and 8 both mention the ACE2 enzyme alongside protein binding verbiage\n",
    "- Topic 5 includes a lot of words about experimental methods \n",
    "- Topic 10 looks like it includes epidemiological information about infection\n",
    "- Topic 9 is probably junk\n",
    "\n",
    "This, or similar, information could be used, for example, to provide a provisional labelling scheme for your papers, or for information extraction - eg finding all the papers that mention bats and infections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing clusters - t-distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to show you this to demonstrate that not everything works, and that is OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_model = TSNE(perplexity = 50, n_iter=1000000, learning_rate=10)\n",
    "features = tsne_model.fit_transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(features[:,0], features[:,1], s=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method can often be really helpful in visualizing extremely high dimensional data, but it is not particularly help in this scenario - it needs a lot of tuning of the adjustable hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  A simple machine learning model: k-means clustering \n",
    "\n",
    "This is one of the simplest unsupervised machine learning algorithms, but illustrates nicely how we can apply such techniques to learn something, and then to predict on something unknown\n",
    "\n",
    "An important note - k-means performs exceedingly badly on high dimensional data due to the <strong>curse of dimensionality</strong>. Thus, we need to reduce the number of dimensions (a lot) before applying this technique. There are many ways of doing this for NLP task, including using deep neural networks, but here we will apply a simpler technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Principal Component Analysis (PCA) to reduce the dimensionality to help k-means perform better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "reduced_matrix = pca.fit_transform(tfidf_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Size of reduced matrix = {reduced_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the k-means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=6)\n",
    "kmeans.fit(reduced_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us see what we're actually getting out of this algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_matrix[:, 0].min() - 0.2, reduced_matrix[:, 0].max() + 0.2\n",
    "y_min, y_max = reduced_matrix[:, 1].min() - 0.2, reduced_matrix[:, 1].max() + 0.2\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(reduced_matrix[:, 0], reduced_matrix[:, 1], 'k.', markersize=2)\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "plt.title('K-means clustering on the digits dataset (PCA-reduced data)\\n'\n",
    "          'Centroids are marked with white cross')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we choose a \"best\" K?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use something called the elbow method\n",
    "distortions = []\n",
    "K = range(1,10)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(reduced_matrix)\n",
    "    kmeanModel.fit(reduced_matrix)\n",
    "    distortions.append(sum(np.min(cdist(reduced_matrix, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / reduced_matrix.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new plot and data\n",
    "plt.plot()\n",
    "colors = ['b', 'g', 'r']\n",
    "markers = ['o', 'v', 's']\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - the 3 or 6-cluster models may be optimal - but be warned about spurious results!\n",
    "\n",
    "Let's look at the PCA <strong>variance explained</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=500)\n",
    "test_matrix = pca.fit_transform(tfidf_matrix.todense())\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that 2 components acutally capture relatively little of the total variance explained, we need several hundred components to effectively do this. This doesn't necessarily invalidate our results, but it does suggest that we should apply some caution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see which papers fall into which cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignment = kmeans.predict(reduced_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see which papers ended up in cluster 1\n",
    "papers_in_cluster1 = cluster_assignment == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_df = df[[\"paper_id\", \"abstract\"]]\n",
    "paper_df.loc[papers_in_cluster1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When all is said and done - the human has to be in the loop to decide whether the clustering and information retrieval we have done makes sense"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda68a29a8857244515aac2d6b75e787442"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
